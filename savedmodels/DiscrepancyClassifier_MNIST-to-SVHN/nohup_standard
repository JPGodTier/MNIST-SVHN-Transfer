nohup: ignoring input
0.0%0.0%0.1%0.1%0.1%0.1%0.1%0.1%0.2%0.2%0.2%0.2%0.2%0.3%0.3%0.3%0.3%0.3%0.3%0.4%0.4%0.4%0.4%0.4%0.5%0.5%0.5%0.5%0.5%0.5%0.6%0.6%0.6%0.6%0.6%0.6%0.7%0.7%0.7%0.7%0.7%0.8%0.8%0.8%0.8%0.8%0.8%0.9%0.9%0.9%0.9%0.9%1.0%1.0%1.0%1.0%1.0%1.0%1.1%1.1%1.1%1.1%1.1%1.2%1.2%1.2%1.2%1.2%1.2%1.3%1.3%1.3%1.3%1.3%1.4%1.4%1.4%1.4%1.4%1.4%1.5%1.5%1.5%1.5%1.5%1.5%1.6%1.6%1.6%1.6%1.6%1.7%1.7%1.7%1.7%1.7%1.7%1.8%1.8%1.8%1.8%1.8%1.9%1.9%1.9%1.9%1.9%1.9%2.0%2.0%2.0%2.0%2.0%2.1%2.1%2.1%2.1%2.1%2.1%2.2%2.2%2.2%2.2%2.2%2.3%2.3%2.3%2.3%2.3%2.3%2.4%2.4%2.4%2.4%2.4%2.4%2.5%2.5%2.5%2.5%2.5%2.6%2.6%2.6%2.6%2.6%2.6%2.7%2.7%2.7%2.7%2.7%2.8%2.8%2.8%2.8%2.8%2.8%2.9%2.9%2.9%2.9%2.9%3.0%3.0%3.0%3.0%3.0%3.0%3.1%3.1%3.1%3.1%3.1%3.2%3.2%3.2%3.2%3.2%3.2%3.3%3.3%3.3%3.3%3.3%3.3%3.4%3.4%3.4%3.4%3.4%3.5%3.5%3.5%3.5%3.5%3.5%3.6%3.6%3.6%3.6%3.6%3.7%3.7%3.7%3.7%3.7%3.7%3.8%3.8%3.8%3.8%3.8%3.9%3.9%3.9%3.9%3.9%3.9%4.0%4.0%4.0%4.0%4.0%4.1%4.1%4.1%4.1%4.1%4.1%4.2%4.2%4.2%4.2%4.2%4.2%4.3%4.3%4.3%4.3%4.3%4.4%4.4%4.4%4.4%4.4%4.4%4.5%4.5%4.5%4.5%4.5%4.6%4.6%4.6%4.6%4.6%4.6%4.7%4.7%4.7%4.7%4.7%4.8%4.8%4.8%4.8%4.8%4.8%4.9%4.9%4.9%4.9%4.9%5.0%5.0%5.0%5.0%5.0%5.0%5.1%5.1%5.1%5.1%5.1%5.1%5.2%5.2%5.2%5.2%5.2%5.3%5.3%5.3%5.3%5.3%5.3%5.4%5.4%5.4%5.4%5.4%5.5%5.5%5.5%5.5%5.5%5.5%5.6%5.6%5.6%5.6%5.6%5.7%5.7%5.7%5.7%5.7%5.7%5.8%5.8%5.8%5.8%5.8%5.9%5.9%5.9%5.9%5.9%5.9%6.0%6.0%6.0%6.0%6.0%6.0%6.1%6.1%6.1%6.1%6.1%6.2%6.2%6.2%6.2%6.2%6.2%6.3%6.3%6.3%6.3%6.3%6.4%6.4%6.4%6.4%6.4%6.4%6.5%6.5%6.5%6.5%6.5%6.6%6.6%6.6%6.6%6.6%6.6%6.7%6.7%6.7%6.7%6.7%6.8%6.8%6.8%6.8%6.8%6.8%6.9%6.9%6.9%6.9%6.9%6.9%7.0%7.0%7.0%7.0%7.0%7.1%7.1%7.1%7.1%7.1%7.1%7.2%7.2%7.2%7.2%7.2%7.3%7.3%7.3%7.3%7.3%7.3%7.4%7.4%7.4%7.4%7.4%7.5%7.5%7.5%7.5%7.5%7.5%7.6%7.6%7.6%7.6%7.6%7.7%7.7%7.7%7.7%7.7%7.7%7.8%7.8%7.8%7.8%7.8%7.8%7.9%7.9%7.9%7.9%7.9%8.0%8.0%8.0%8.0%8.0%8.0%8.1%8.1%8.1%8.1%8.1%8.2%8.2%8.2%8.2%8.2%8.2%8.3%8.3%8.3%8.3%8.3%8.4%8.4%8.4%8.4%8.4%8.4%8.5%8.5%8.5%8.5%8.5%8.6%8.6%8.6%8.6%8.6%8.6%8.7%8.7%8.7%8.7%8.7%8.7%8.8%8.8%8.8%8.8%8.8%8.9%8.9%8.9%8.9%8.9%8.9%9.0%9.0%9.0%9.0%9.0%9.1%9.1%9.1%9.1%9.1%9.1%9.2%9.2%9.2%9.2%9.2%9.3%9.3%9.3%9.3%9.3%9.3%9.4%9.4%9.4%9.4%9.4%9.5%9.5%9.5%9.5%9.5%9.5%9.6%9.6%9.6%9.6%9.6%9.6%9.7%9.7%9.7%9.7%9.7%9.8%9.8%9.8%9.8%9.8%9.8%9.9%9.9%9.9%9.9%9.9%10.0%10.0%10.0%10.0%10.0%10.0%10.1%10.1%10.1%10.1%10.1%10.2%10.2%10.2%10.2%10.2%10.2%10.3%10.3%10.3%10.3%10.3%10.4%10.4%10.4%10.4%10.4%10.4%10.5%10.5%10.5%10.5%10.5%10.5%10.6%10.6%10.6%10.6%10.6%10.7%10.7%10.7%10.7%10.7%10.7%10.8%10.8%10.8%10.8%10.8%10.9%10.9%10.9%10.9%10.9%10.9%11.0%11.0%11.0%11.0%11.0%11.1%11.1%11.1%11.1%11.1%11.1%11.2%11.2%11.2%11.2%11.2%11.3%11.3%11.3%11.3%11.3%11.3%11.4%11.4%11.4%11.4%11.4%11.4%11.5%11.5%11.5%11.5%11.5%11.6%11.6%11.6%11.6%11.6%11.6%11.7%11.7%11.7%11.7%11.7%11.8%11.8%11.8%11.8%11.8%11.8%11.9%11.9%11.9%11.9%11.9%12.0%12.0%12.0%12.0%12.0%12.0%12.1%12.1%12.1%12.1%12.1%12.2%12.2%12.2%12.2%12.2%12.2%12.3%12.3%12.3%12.3%12.3%12.3%12.4%12.4%12.4%12.4%12.4%12.5%12.5%12.5%12.5%12.5%12.5%12.6%12.6%12.6%12.6%12.6%12.7%12.7%12.7%12.7%12.7%12.7%12.8%12.8%12.8%12.8%12.8%12.9%12.9%12.9%12.9%12.9%12.9%13.0%13.0%13.0%13.0%13.0%13.1%13.1%13.1%13.1%13.1%13.1%13.2%13.2%13.2%13.2%13.2%13.2%13.3%13.3%13.3%13.3%13.3%13.4%13.4%13.4%13.4%13.4%13.4%13.5%13.5%13.5%13.5%13.5%13.6%13.6%13.6%13.6%13.6%13.6%13.7%13.7%13.7%13.7%13.7%13.8%13.8%13.8%13.8%13.8%13.8%13.9%13.9%13.9%13.9%13.9%14.0%14.0%14.0%14.0%14.0%14.0%14.1%14.1%14.1%14.1%14.1%14.1%14.2%14.2%14.2%14.2%14.2%14.3%14.3%14.3%14.3%14.3%14.3%14.4%14.4%14.4%14.4%14.4%14.5%14.5%14.5%14.5%14.5%14.5%14.6%14.6%14.6%14.6%14.6%14.7%14.7%14.7%14.7%14.7%14.7%14.8%14.8%14.8%14.8%14.8%14.9%14.9%14.9%14.9%14.9%14.9%15.0%15.0%15.0%15.0%15.0%15.0%15.1%15.1%15.1%15.1%15.1%15.2%15.2%15.2%15.2%15.2%15.2%15.3%15.3%15.3%15.3%15.3%15.4%15.4%15.4%15.4%15.4%15.4%15.5%15.5%15.5%15.5%15.5%15.6%15.6%15.6%15.6%15.6%15.6%15.7%15.7%15.7%15.7%15.7%15.8%15.8%15.8%15.8%15.8%15.8%15.9%15.9%15.9%15.9%15.9%15.9%16.0%16.0%16.0%16.0%16.0%16.1%16.1%16.1%16.1%16.1%16.1%16.2%16.2%16.2%16.2%16.2%16.3%16.3%16.3%16.3%16.3%16.3%16.4%16.4%16.4%16.4%16.4%16.5%16.5%16.5%16.5%16.5%16.5%16.6%16.6%16.6%16.6%16.6%16.7%16.7%16.7%16.7%16.7%16.7%16.8%16.8%16.8%16.8%16.8%16.8%16.9%16.9%16.9%16.9%16.9%17.0%17.0%17.0%17.0%17.0%17.0%17.1%17.1%17.1%17.1%17.1%17.2%17.2%17.2%17.2%17.2%17.2%17.3%17.3%17.3%17.3%17.3%17.4%17.4%17.4%17.4%17.4%17.4%17.5%17.5%17.5%17.5%17.5%17.6%17.6%17.6%17.6%17.6%17.6%17.7%17.7%17.7%17.7%17.7%17.7%17.8%17.8%17.8%17.8%17.8%17.9%17.9%17.9%17.9%17.9%17.9%18.0%18.0%18.0%18.0%18.0%18.1%18.1%18.1%18.1%18.1%18.1%18.2%18.2%18.2%18.2%18.2%18.3%18.3%18.3%18.3%18.3%18.3%18.4%18.4%18.4%18.4%18.4%18.5%18.5%18.5%18.5%18.5%18.5%18.6%18.6%18.6%18.6%18.6%18.6%18.7%18.7%18.7%18.7%18.7%18.8%18.8%18.8%18.8%18.8%18.8%18.9%18.9%18.9%18.9%18.9%19.0%19.0%19.0%19.0%19.0%19.0%19.1%19.1%19.1%19.1%19.1%19.2%19.2%19.2%19.2%19.2%19.2%19.3%19.3%19.3%19.3%19.3%19.4%19.4%19.4%19.4%19.4%19.4%19.5%19.5%19.5%19.5%19.5%19.5%19.6%19.6%19.6%19.6%19.6%19.7%19.7%19.7%19.7%19.7%19.7%19.8%19.8%19.8%19.8%19.8%19.9%19.9%19.9%19.9%19.9%19.9%20.0%20.0%20.0%20.0%20.0%20.1%20.1%20.1%20.1%20.1%20.1%20.2%20.2%20.2%20.2%20.2%20.3%20.3%20.3%20.3%20.3%20.3%20.4%20.4%20.4%20.4%20.4%20.4%20.5%20.5%20.5%20.5%20.5%20.6%20.6%20.6%20.6%20.6%20.6%20.7%20.7%20.7%20.7%20.7%20.8%20.8%20.8%20.8%20.8%20.8%20.9%20.9%20.9%20.9%20.9%21.0%21.0%21.0%21.0%21.0%21.0%21.1%21.1%21.1%21.1%21.1%21.2%21.2%21.2%21.2%21.2%21.2%21.3%21.3%21.3%21.3%21.3%21.3%21.4%21.4%21.4%21.4%21.4%21.5%21.5%21.5%21.5%21.5%21.5%21.6%21.6%21.6%21.6%21.6%21.7%21.7%21.7%21.7%21.7%21.7%21.8%21.8%21.8%21.8%21.8%21.9%21.9%21.9%21.9%21.9%21.9%22.0%22.0%22.0%22.0%22.0%22.1%22.1%22.1%22.1%22.1%22.1%22.2%22.2%22.2%22.2%22.2%22.2%22.3%22.3%22.3%22.3%22.3%22.4%22.4%22.4%22.4%22.4%22.4%22.5%22.5%22.5%22.5%22.5%22.6%22.6%22.6%22.6%22.6%22.6%22.7%22.7%22.7%22.7%22.7%22.8%22.8%22.8%22.8%22.8%22.8%22.9%22.9%22.9%22.9%22.9%23.0%23.0%23.0%23.0%23.0%23.0%23.1%23.1%23.1%23.1%23.1%23.1%23.2%23.2%23.2%23.2%23.2%23.3%23.3%23.3%23.3%23.3%23.3%23.4%23.4%23.4%23.4%23.4%23.5%23.5%23.5%23.5%23.5%23.5%23.6%23.6%23.6%23.6%23.6%23.7%23.7%23.7%23.7%23.7%23.7%23.8%23.8%23.8%23.8%23.8%23.9%23.9%23.9%23.9%23.9%23.9%24.0%24.0%24.0%24.0%24.0%24.0%24.1%24.1%24.1%24.1%24.1%24.2%24.2%24.2%24.2%24.2%24.2%24.3%24.3%24.3%24.3%24.3%24.4%24.4%24.4%24.4%24.4%24.4%24.5%24.5%24.5%24.5%24.5%24.6%24.6%24.6%24.6%24.6%24.6%24.7%24.7%24.7%24.7%24.7%24.8%24.8%24.8%24.8%24.8%24.8%24.9%24.9%24.9%24.9%24.9%24.9%25.0%25.0%25.0%25.0%25.0%25.1%25.1%25.1%25.1%25.1%25.1%25.2%25.2%25.2%25.2%25.2%25.3%25.3%25.3%25.3%25.3%25.3%25.4%25.4%25.4%25.4%25.4%25.5%25.5%25.5%25.5%25.5%25.5%25.6%25.6%25.6%25.6%25.6%25.7%25.7%25.7%25.7%25.7%25.7%25.8%25.8%25.8%25.8%25.8%25.8%25.9%25.9%25.9%25.9%25.9%26.0%26.0%26.0%26.0%26.0%26.0%26.1%26.1%26.1%26.1%26.1%26.2%26.2%26.2%26.2%26.2%26.2%26.3%26.3%26.3%26.3%26.3%26.4%26.4%26.4%26.4%26.4%26.4%26.5%26.5%26.5%26.5%26.5%26.6%26.6%26.6%26.6%26.6%26.6%26.7%26.7%26.7%26.7%26.7%26.7%26.8%26.8%26.8%26.8%26.8%26.9%26.9%26.9%26.9%26.9%26.9%27.0%27.0%27.0%27.0%27.0%27.1%27.1%27.1%27.1%27.1%27.1%27.2%27.2%27.2%27.2%27.2%27.3%27.3%27.3%27.3%27.3%27.3%27.4%27.4%27.4%27.4%27.4%27.5%27.5%27.5%27.5%27.5%27.5%27.6%27.6%27.6%27.6%27.6%27.6%27.7%27.7%27.7%27.7%27.7%27.8%27.8%27.8%27.8%27.8%27.8%27.9%27.9%27.9%27.9%27.9%28.0%28.0%28.0%28.0%28.0%28.0%28.1%28.1%28.1%28.1%28.1%28.2%28.2%28.2%28.2%28.2%28.2%28.3%28.3%28.3%28.3%28.3%28.4%28.4%28.4%28.4%28.4%28.4%28.5%28.5%28.5%28.5%28.5%28.5%28.6%28.6%28.6%28.6%28.6%28.7%28.7%28.7%28.7%28.7%28.7%28.8%28.8%28.8%28.8%28.8%28.9%28.9%28.9%28.9%28.9%28.9%29.0%29.0%29.0%29.0%29.0%29.1%29.1%29.1%29.1%29.1%29.1%29.2%29.2%29.2%29.2%29.2%29.3%29.3%29.3%29.3%29.3%29.3%29.4%29.4%29.4%29.4%29.4%29.4%29.5%29.5%29.5%29.5%29.5%29.6%29.6%29.6%29.6%29.6%29.6%29.7%29.7%29.7%29.7%29.7%29.8%29.8%29.8%29.8%29.8%29.8%29.9%29.9%29.9%29.9%29.9%30.0%30.0%30.0%30.0%30.0%30.0%30.1%30.1%30.1%30.1%30.1%30.2%30.2%30.2%30.2%30.2%30.2%30.3%30.3%30.3%30.3%30.3%30.3%30.4%30.4%30.4%30.4%30.4%30.5%30.5%30.5%30.5%30.5%30.5%30.6%30.6%30.6%30.6%30.6%30.7%30.7%30.7%30.7%30.7%30.7%30.8%30.8%30.8%30.8%30.8%30.9%30.9%30.9%30.9%30.9%30.9%31.0%31.0%31.0%31.0%31.0%31.1%31.1%31.1%31.1%31.1%31.1%31.2%31.2%31.2%31.2%31.2%31.2%31.3%31.3%31.3%31.3%31.3%31.4%31.4%31.4%31.4%31.4%31.4%31.5%31.5%31.5%31.5%31.5%31.6%31.6%31.6%31.6%31.6%31.6%31.7%31.7%31.7%31.7%31.7%31.8%31.8%31.8%31.8%31.8%31.8%31.9%31.9%31.9%31.9%31.9%32.0%32.0%32.0%32.0%32.0%32.0%32.1%32.1%32.1%32.1%32.1%32.1%32.2%32.2%32.2%32.2%32.2%32.3%32.3%32.3%32.3%32.3%32.3%32.4%32.4%32.4%32.4%32.4%32.5%32.5%32.5%32.5%32.5%32.5%32.6%32.6%32.6%32.6%32.6%32.7%32.7%32.7%32.7%32.7%32.7%32.8%32.8%32.8%32.8%32.8%32.9%32.9%32.9%32.9%32.9%32.9%33.0%33.0%33.0%33.0%33.0%33.0%33.1%33.1%33.1%33.1%33.1%33.2%33.2%33.2%33.2%33.2%33.2%33.3%33.3%33.3%33.3%33.3%33.4%33.4%33.4%33.4%33.4%33.4%33.5%33.5%33.5%33.5%33.5%33.6%33.6%33.6%33.6%33.6%33.6%33.7%33.7%33.7%33.7%33.7%33.8%33.8%33.8%33.8%33.8%33.8%33.9%33.9%33.9%33.9%33.9%33.9%34.0%34.0%34.0%34.0%34.0%34.1%34.1%34.1%34.1%34.1%34.1%34.2%34.2%34.2%34.2%34.2%34.3%34.3%34.3%34.3%34.3%34.3%34.4%34.4%34.4%34.4%34.4%34.5%34.5%34.5%34.5%34.5%34.5%34.6%34.6%34.6%34.6%34.6%34.7%34.7%34.7%34.7%34.7%34.7%34.8%34.8%34.8%34.8%34.8%34.8%34.9%34.9%34.9%34.9%34.9%35.0%35.0%35.0%35.0%35.0%35.0%35.1%35.1%35.1%35.1%35.1%35.2%35.2%35.2%35.2%35.2%35.2%35.3%35.3%35.3%35.3%35.3%35.4%35.4%35.4%35.4%35.4%35.4%35.5%35.5%35.5%35.5%35.5%35.6%35.6%35.6%35.6%35.6%35.6%35.7%35.7%35.7%35.7%35.7%35.7%35.8%35.8%35.8%35.8%35.8%35.9%35.9%35.9%35.9%35.9%35.9%36.0%36.0%36.0%36.0%36.0%36.1%36.1%36.1%36.1%36.1%36.1%36.2%36.2%36.2%36.2%36.2%36.3%36.3%36.3%36.3%36.3%36.3%36.4%36.4%36.4%36.4%36.4%36.5%36.5%36.5%36.5%36.5%36.5%36.6%36.6%36.6%36.6%36.6%36.6%36.7%36.7%36.7%36.7%36.7%36.8%36.8%36.8%36.8%36.8%36.8%36.9%36.9%36.9%36.9%36.9%37.0%37.0%37.0%37.0%37.0%37.0%37.1%37.1%37.1%37.1%37.1%37.2%37.2%37.2%37.2%37.2%37.2%37.3%37.3%37.3%37.3%37.3%37.4%37.4%37.4%37.4%37.4%37.4%37.5%37.5%37.5%37.5%37.5%37.5%37.6%37.6%37.6%37.6%37.6%37.7%37.7%37.7%37.7%37.7%37.7%37.8%37.8%37.8%37.8%37.8%37.9%37.9%37.9%37.9%37.9%37.9%38.0%38.0%38.0%38.0%38.0%38.1%38.1%38.1%38.1%38.1%38.1%38.2%38.2%38.2%38.2%38.2%38.3%38.3%38.3%38.3%38.3%38.3%38.4%38.4%38.4%38.4%38.4%38.4%38.5%38.5%38.5%38.5%38.5%38.6%38.6%38.6%38.6%38.6%38.6%38.7%38.7%38.7%38.7%38.7%38.8%38.8%38.8%38.8%38.8%38.8%38.9%38.9%38.9%38.9%38.9%39.0%39.0%39.0%39.0%39.0%39.0%39.1%39.1%39.1%39.1%39.1%39.2%39.2%39.2%39.2%39.2%39.2%39.3%39.3%39.3%39.3%39.3%39.3%39.4%39.4%39.4%39.4%39.4%39.5%39.5%39.5%39.5%39.5%39.5%39.6%39.6%39.6%39.6%39.6%39.7%39.7%39.7%39.7%39.7%39.7%39.8%39.8%39.8%39.8%39.8%39.9%39.9%39.9%39.9%39.9%39.9%40.0%40.0%40.0%40.0%40.0%40.1%40.1%40.1%40.1%40.1%40.1%40.2%40.2%40.2%40.2%40.2%40.2%40.3%40.3%40.3%40.3%40.3%40.4%40.4%40.4%40.4%40.4%40.4%40.5%40.5%40.5%40.5%40.5%40.6%40.6%40.6%40.6%40.6%40.6%40.7%40.7%40.7%40.7%40.7%40.8%40.8%40.8%40.8%40.8%40.8%40.9%40.9%40.9%40.9%40.9%41.0%41.0%41.0%41.0%41.0%41.0%41.1%41.1%41.1%41.1%41.1%41.1%41.2%41.2%41.2%41.2%41.2%41.3%41.3%41.3%41.3%41.3%41.3%41.4%41.4%41.4%41.4%41.4%41.5%41.5%41.5%41.5%41.5%41.5%41.6%41.6%41.6%41.6%41.6%41.7%41.7%41.7%41.7%41.7%41.7%41.8%41.8%41.8%41.8%41.8%41.9%41.9%41.9%41.9%41.9%41.9%42.0%42.0%42.0%42.0%42.0%42.0%42.1%42.1%42.1%42.1%42.1%42.2%42.2%42.2%42.2%42.2%42.2%42.3%42.3%42.3%42.3%42.3%42.4%42.4%42.4%42.4%42.4%42.4%42.5%42.5%42.5%42.5%42.5%42.6%42.6%42.6%42.6%42.6%42.6%42.7%42.7%42.7%42.7%42.7%42.8%42.8%42.8%42.8%42.8%42.8%42.9%42.9%42.9%42.9%42.9%42.9%43.0%43.0%43.0%43.0%43.0%43.1%43.1%43.1%43.1%43.1%43.1%43.2%43.2%43.2%43.2%43.2%43.3%43.3%43.3%43.3%43.3%43.3%43.4%43.4%43.4%43.4%43.4%43.5%43.5%43.5%43.5%43.5%43.5%43.6%43.6%43.6%43.6%43.6%43.7%43.7%43.7%43.7%43.7%43.7%43.8%43.8%43.8%43.8%43.8%43.8%43.9%43.9%43.9%43.9%43.9%44.0%44.0%44.0%44.0%44.0%44.0%44.1%44.1%44.1%44.1%44.1%44.2%44.2%44.2%44.2%44.2%44.2%44.3%44.3%44.3%44.3%44.3%44.4%44.4%44.4%44.4%44.4%44.4%44.5%44.5%44.5%44.5%44.5%44.6%44.6%44.6%44.6%44.6%44.6%44.7%44.7%44.7%44.7%44.7%44.7%44.8%44.8%44.8%44.8%44.8%44.9%44.9%44.9%44.9%44.9%44.9%45.0%45.0%45.0%45.0%45.0%45.1%45.1%45.1%45.1%45.1%45.1%45.2%45.2%45.2%45.2%45.2%45.3%45.3%45.3%45.3%45.3%45.3%45.4%45.4%45.4%45.4%45.4%45.5%45.5%45.5%45.5%45.5%45.5%45.6%45.6%45.6%45.6%45.6%45.6%45.7%45.7%45.7%45.7%45.7%45.8%45.8%45.8%45.8%45.8%45.8%45.9%45.9%45.9%45.9%45.9%46.0%46.0%46.0%46.0%46.0%46.0%46.1%46.1%46.1%46.1%46.1%46.2%46.2%46.2%46.2%46.2%46.2%46.3%46.3%46.3%46.3%46.3%46.4%46.4%46.4%46.4%46.4%46.4%46.5%46.5%46.5%46.5%46.5%46.5%46.6%46.6%46.6%46.6%46.6%46.7%46.7%46.7%46.7%46.7%46.7%46.8%46.8%46.8%46.8%46.8%46.9%46.9%46.9%46.9%46.9%46.9%47.0%47.0%47.0%47.0%47.0%47.1%47.1%47.1%47.1%47.1%47.1%47.2%47.2%47.2%47.2%47.2%47.3%47.3%47.3%47.3%47.3%47.3%47.4%47.4%47.4%47.4%47.4%47.4%47.5%47.5%47.5%47.5%47.5%47.6%47.6%47.6%47.6%47.6%47.6%47.7%47.7%47.7%47.7%47.7%47.8%47.8%47.8%47.8%47.8%47.8%47.9%47.9%47.9%47.9%47.9%48.0%48.0%48.0%48.0%48.0%48.0%48.1%48.1%48.1%48.1%48.1%48.2%48.2%48.2%48.2%48.2%48.2%48.3%48.3%48.3%48.3%48.3%48.3%48.4%48.4%48.4%48.4%48.4%48.5%48.5%48.5%48.5%48.5%48.5%48.6%48.6%48.6%48.6%48.6%48.7%48.7%48.7%48.7%48.7%48.7%48.8%48.8%48.8%48.8%48.8%48.9%48.9%48.9%48.9%48.9%48.9%49.0%49.0%49.0%49.0%49.0%49.1%49.1%49.1%49.1%49.1%49.1%49.2%49.2%49.2%49.2%49.2%49.2%49.3%49.3%49.3%49.3%49.3%49.4%49.4%49.4%49.4%49.4%49.4%49.5%49.5%49.5%49.5%49.5%49.6%49.6%49.6%49.6%49.6%49.6%49.7%49.7%49.7%49.7%49.7%49.8%49.8%49.8%49.8%49.8%49.8%49.9%49.9%49.9%49.9%49.9%50.0%50.0%50.0%50.0%50.0%50.0%50.1%50.1%50.1%50.1%50.1%50.1%50.2%50.2%50.2%50.2%50.2%50.3%50.3%50.3%50.3%50.3%50.3%50.4%50.4%50.4%50.4%50.4%50.5%50.5%50.5%50.5%50.5%50.5%50.6%50.6%50.6%50.6%50.6%50.7%50.7%50.7%50.7%50.7%50.7%50.8%50.8%50.8%50.8%50.8%50.9%50.9%50.9%50.9%50.9%50.9%51.0%51.0%51.0%51.0%51.0%51.0%51.1%51.1%51.1%51.1%51.1%51.2%51.2%51.2%51.2%51.2%51.2%51.3%51.3%51.3%51.3%51.3%51.4%51.4%51.4%51.4%51.4%51.4%51.5%51.5%51.5%51.5%51.5%51.6%51.6%51.6%51.6%51.6%51.6%51.7%51.7%51.7%51.7%51.7%51.8%51.8%51.8%51.8%51.8%51.8%51.9%51.9%51.9%51.9%51.9%51.9%52.0%52.0%52.0%52.0%52.0%52.1%52.1%52.1%52.1%52.1%52.1%52.2%52.2%52.2%52.2%52.2%52.3%52.3%52.3%52.3%52.3%52.3%52.4%52.4%52.4%52.4%52.4%52.5%52.5%52.5%52.5%52.5%52.5%52.6%52.6%52.6%52.6%52.6%52.7%52.7%52.7%52.7%52.7%52.7%52.8%52.8%52.8%52.8%52.8%52.8%52.9%52.9%52.9%52.9%52.9%53.0%53.0%53.0%53.0%53.0%53.0%53.1%53.1%53.1%53.1%53.1%53.2%53.2%53.2%53.2%53.2%53.2%53.3%53.3%53.3%53.3%53.3%53.4%53.4%53.4%53.4%53.4%53.4%53.5%53.5%53.5%53.5%53.5%53.6%53.6%53.6%53.6%53.6%53.6%53.7%53.7%53.7%53.7%53.7%53.7%53.8%53.8%53.8%53.8%53.8%53.9%53.9%53.9%53.9%53.9%53.9%54.0%54.0%54.0%54.0%54.0%54.1%54.1%54.1%54.1%54.1%54.1%54.2%54.2%54.2%54.2%54.2%54.3%54.3%54.3%54.3%54.3%54.3%54.4%54.4%54.4%54.4%54.4%54.5%54.5%54.5%54.5%54.5%54.5%54.6%54.6%54.6%54.6%54.6%54.6%54.7%54.7%54.7%54.7%54.7%54.8%54.8%54.8%54.8%54.8%54.8%54.9%54.9%54.9%54.9%54.9%55.0%55.0%55.0%55.0%55.0%55.0%55.1%55.1%55.1%55.1%55.1%55.2%55.2%55.2%55.2%55.2%55.2%55.3%55.3%55.3%55.3%55.3%55.4%55.4%55.4%55.4%55.4%55.4%55.5%55.5%55.5%55.5%55.5%55.5%55.6%55.6%55.6%55.6%55.6%55.7%55.7%55.7%55.7%55.7%55.7%55.8%55.8%55.8%55.8%55.8%55.9%55.9%55.9%55.9%55.9%55.9%56.0%56.0%56.0%56.0%56.0%56.1%56.1%56.1%56.1%56.1%56.1%56.2%56.2%56.2%56.2%56.2%56.3%56.3%56.3%56.3%56.3%56.3%56.4%56.4%56.4%56.4%56.4%56.4%56.5%56.5%56.5%56.5%56.5%56.6%56.6%56.6%56.6%56.6%56.6%56.7%56.7%56.7%56.7%56.7%56.8%56.8%56.8%56.8%56.8%56.8%56.9%56.9%56.9%56.9%56.9%57.0%57.0%57.0%57.0%57.0%57.0%57.1%57.1%57.1%57.1%57.1%57.2%57.2%57.2%57.2%57.2%57.2%57.3%57.3%57.3%57.3%57.3%57.3%57.4%57.4%57.4%57.4%57.4%57.5%57.5%57.5%57.5%57.5%57.5%57.6%57.6%57.6%57.6%57.6%57.7%57.7%57.7%57.7%57.7%57.7%57.8%57.8%57.8%57.8%57.8%57.9%57.9%57.9%57.9%57.9%57.9%58.0%58.0%58.0%58.0%58.0%58.1%58.1%58.1%58.1%58.1%58.1%58.2%58.2%58.2%58.2%58.2%58.2%58.3%58.3%58.3%58.3%58.3%58.4%58.4%58.4%58.4%58.4%58.4%58.5%58.5%58.5%58.5%58.5%58.6%58.6%58.6%58.6%58.6%58.6%58.7%58.7%58.7%58.7%58.7%58.8%58.8%58.8%58.8%58.8%58.8%58.9%58.9%58.9%58.9%58.9%59.0%59.0%59.0%59.0%59.0%59.0%59.1%59.1%59.1%59.1%59.1%59.1%59.2%59.2%59.2%59.2%59.2%59.3%59.3%59.3%59.3%59.3%59.3%59.4%59.4%59.4%59.4%59.4%59.5%59.5%59.5%59.5%59.5%59.5%59.6%59.6%59.6%59.6%59.6%59.7%59.7%59.7%59.7%59.7%59.7%59.8%59.8%59.8%59.8%59.8%59.9%59.9%59.9%59.9%59.9%59.9%60.0%60.0%60.0%60.0%60.0%60.0%60.1%60.1%60.1%60.1%60.1%60.2%60.2%60.2%60.2%60.2%60.2%60.3%60.3%60.3%60.3%60.3%60.4%60.4%60.4%60.4%60.4%60.4%60.5%60.5%60.5%60.5%60.5%60.6%60.6%60.6%60.6%60.6%60.6%60.7%60.7%60.7%60.7%60.7%60.8%60.8%60.8%60.8%60.8%60.8%60.9%60.9%60.9%60.9%60.9%60.9%61.0%61.0%61.0%61.0%61.0%61.1%61.1%61.1%61.1%61.1%61.1%61.2%61.2%61.2%61.2%61.2%61.3%61.3%61.3%61.3%61.3%61.3%61.4%61.4%61.4%61.4%61.4%61.5%61.5%61.5%61.5%61.5%61.5%61.6%61.6%61.6%61.6%61.6%61.7%61.7%61.7%61.7%61.7%61.7%61.8%61.8%61.8%61.8%61.8%61.8%61.9%61.9%61.9%61.9%61.9%62.0%62.0%62.0%62.0%62.0%62.0%62.1%62.1%62.1%62.1%62.1%62.2%62.2%62.2%62.2%62.2%62.2%62.3%62.3%62.3%62.3%62.3%62.4%62.4%62.4%62.4%62.4%62.4%62.5%62.5%62.5%62.5%62.5%62.6%62.6%62.6%62.6%62.6%62.6%62.7%62.7%62.7%62.7%62.7%62.7%62.8%62.8%62.8%62.8%62.8%62.9%62.9%62.9%62.9%62.9%62.9%63.0%63.0%63.0%63.0%63.0%63.1%63.1%63.1%63.1%63.1%63.1%63.2%63.2%63.2%63.2%63.2%63.3%63.3%63.3%63.3%63.3%63.3%63.4%63.4%63.4%63.4%63.4%63.5%63.5%63.5%63.5%63.5%63.5%63.6%63.6%63.6%63.6%63.6%63.6%63.7%63.7%63.7%63.7%63.7%63.8%63.8%63.8%63.8%63.8%63.8%63.9%63.9%63.9%63.9%63.9%64.0%64.0%64.0%64.0%64.0%64.0%64.1%64.1%64.1%64.1%64.1%64.2%64.2%64.2%64.2%64.2%64.2%64.3%64.3%64.3%64.3%64.3%64.4%64.4%64.4%64.4%64.4%64.4%64.5%64.5%64.5%64.5%64.5%64.5%64.6%64.6%64.6%64.6%64.6%64.7%64.7%64.7%64.7%64.7%64.7%64.8%64.8%64.8%64.8%64.8%64.9%64.9%64.9%64.9%64.9%64.9%65.0%65.0%65.0%65.0%65.0%65.1%65.1%65.1%65.1%65.1%65.1%65.2%65.2%65.2%65.2%65.2%65.3%65.3%65.3%65.3%65.3%65.3%65.4%65.4%65.4%65.4%65.4%65.4%65.5%65.5%65.5%65.5%65.5%65.6%65.6%65.6%65.6%65.6%65.6%65.7%65.7%65.7%65.7%65.7%65.8%65.8%65.8%65.8%65.8%65.8%65.9%65.9%65.9%65.9%65.9%66.0%66.0%66.0%66.0%66.0%66.0%66.1%66.1%66.1%66.1%66.1%66.2%66.2%66.2%66.2%66.2%66.2%66.3%66.3%66.3%66.3%66.3%66.3%66.4%66.4%66.4%66.4%66.4%66.5%66.5%66.5%66.5%66.5%66.5%66.6%66.6%66.6%66.6%66.6%66.7%66.7%66.7%66.7%66.7%66.7%66.8%66.8%66.8%66.8%66.8%66.9%66.9%66.9%66.9%66.9%66.9%67.0%67.0%67.0%67.0%67.0%67.1%67.1%67.1%67.1%67.1%67.1%67.2%67.2%67.2%67.2%67.2%67.2%67.3%67.3%67.3%67.3%67.3%67.4%67.4%67.4%67.4%67.4%67.4%67.5%67.5%67.5%67.5%67.5%67.6%67.6%67.6%67.6%67.6%67.6%67.7%67.7%67.7%67.7%67.7%67.8%67.8%67.8%67.8%67.8%67.8%67.9%67.9%67.9%67.9%67.9%68.0%68.0%68.0%68.0%68.0%68.0%68.1%68.1%68.1%68.1%68.1%68.1%68.2%68.2%68.2%68.2%68.2%68.3%68.3%68.3%68.3%68.3%68.3%68.4%68.4%68.4%68.4%68.4%68.5%68.5%68.5%68.5%68.5%68.5%68.6%68.6%68.6%68.6%68.6%68.7%68.7%68.7%68.7%68.7%68.7%68.8%68.8%68.8%68.8%68.8%68.9%68.9%68.9%68.9%68.9%68.9%69.0%69.0%69.0%69.0%69.0%69.0%69.1%69.1%69.1%69.1%69.1%69.2%69.2%69.2%69.2%69.2%69.2%69.3%69.3%69.3%69.3%69.3%69.4%69.4%69.4%69.4%69.4%69.4%69.5%69.5%69.5%69.5%69.5%69.6%69.6%69.6%69.6%69.6%69.6%69.7%69.7%69.7%69.7%69.7%69.8%69.8%69.8%69.8%69.8%69.8%69.9%69.9%69.9%69.9%69.9%69.9%70.0%70.0%70.0%70.0%70.0%70.1%70.1%70.1%70.1%70.1%70.1%70.2%70.2%70.2%70.2%70.2%70.3%70.3%70.3%70.3%70.3%70.3%70.4%70.4%70.4%70.4%70.4%70.5%70.5%70.5%70.5%70.5%70.5%70.6%70.6%70.6%70.6%70.6%70.7%70.7%70.7%70.7%70.7%70.7%70.8%70.8%70.8%70.8%70.8%70.8%70.9%70.9%70.9%70.9%70.9%71.0%71.0%71.0%71.0%71.0%71.0%71.1%71.1%71.1%71.1%71.1%71.2%71.2%71.2%71.2%71.2%71.2%71.3%71.3%71.3%71.3%71.3%71.4%71.4%71.4%71.4%71.4%71.4%71.5%71.5%71.5%71.5%71.5%71.6%71.6%71.6%71.6%71.6%71.6%71.7%71.7%71.7%71.7%71.7%71.7%71.8%71.8%71.8%71.8%71.8%71.9%71.9%71.9%71.9%71.9%71.9%72.0%72.0%72.0%72.0%72.0%72.1%72.1%72.1%72.1%72.1%72.1%72.2%72.2%72.2%72.2%72.2%72.3%72.3%72.3%72.3%72.3%72.3%72.4%72.4%72.4%72.4%72.4%72.5%72.5%72.5%72.5%72.5%72.5%72.6%72.6%72.6%72.6%72.6%72.6%72.7%72.7%72.7%72.7%72.7%72.8%72.8%72.8%72.8%72.8%72.8%72.9%72.9%72.9%72.9%72.9%73.0%73.0%73.0%73.0%73.0%73.0%73.1%73.1%73.1%73.1%73.1%73.2%73.2%73.2%73.2%73.2%73.2%73.3%73.3%73.3%73.3%73.3%73.4%73.4%73.4%73.4%73.4%73.4%73.5%73.5%73.5%73.5%73.5%73.5%73.6%73.6%73.6%73.6%73.6%73.7%73.7%73.7%73.7%73.7%73.7%73.8%73.8%73.8%73.8%73.8%73.9%73.9%73.9%73.9%73.9%73.9%74.0%74.0%74.0%74.0%74.0%74.1%74.1%74.1%74.1%74.1%74.1%74.2%74.2%74.2%74.2%74.2%74.3%74.3%74.3%74.3%74.3%74.3%74.4%74.4%74.4%74.4%74.4%74.4%74.5%74.5%74.5%74.5%74.5%74.6%74.6%74.6%74.6%74.6%74.6%74.7%74.7%74.7%74.7%74.7%74.8%74.8%74.8%74.8%74.8%74.8%74.9%74.9%74.9%74.9%74.9%75.0%75.0%75.0%75.0%75.0%75.0%75.1%75.1%75.1%75.1%75.1%75.2%75.2%75.2%75.2%75.2%75.2%75.3%75.3%75.3%75.3%75.3%75.3%75.4%75.4%75.4%75.4%75.4%75.5%75.5%75.5%75.5%75.5%75.5%75.6%75.6%75.6%75.6%75.6%75.7%75.7%75.7%75.7%75.7%75.7%75.8%75.8%75.8%75.8%75.8%75.9%75.9%75.9%75.9%75.9%75.9%76.0%76.0%76.0%76.0%76.0%76.1%76.1%76.1%76.1%76.1%76.1%76.2%76.2%76.2%76.2%76.2%76.2%76.3%76.3%76.3%76.3%76.3%76.4%76.4%76.4%76.4%76.4%76.4%76.5%76.5%76.5%76.5%76.5%76.6%76.6%76.6%76.6%76.6%76.6%76.7%76.7%76.7%76.7%76.7%76.8%76.8%76.8%76.8%76.8%76.8%76.9%76.9%76.9%76.9%76.9%77.0%77.0%77.0%77.0%77.0%77.0%77.1%77.1%77.1%77.1%77.1%77.1%77.2%77.2%77.2%77.2%77.2%77.3%77.3%77.3%77.3%77.3%77.3%77.4%77.4%77.4%77.4%77.4%77.5%77.5%77.5%77.5%77.5%77.5%77.6%77.6%77.6%77.6%77.6%77.7%77.7%77.7%77.7%77.7%77.7%77.8%77.8%77.8%77.8%77.8%77.9%77.9%77.9%77.9%77.9%77.9%78.0%78.0%78.0%78.0%78.0%78.0%78.1%78.1%78.1%78.1%78.1%78.2%78.2%78.2%78.2%78.2%78.2%78.3%78.3%78.3%78.3%78.3%78.4%78.4%78.4%78.4%78.4%78.4%78.5%78.5%78.5%78.5%78.5%78.6%78.6%78.6%78.6%78.6%78.6%78.7%78.7%78.7%78.7%78.7%78.8%78.8%78.8%78.8%78.8%78.8%78.9%78.9%78.9%78.9%78.9%78.9%79.0%79.0%79.0%79.0%79.0%79.1%79.1%79.1%79.1%79.1%79.1%79.2%79.2%79.2%79.2%79.2%79.3%79.3%79.3%79.3%79.3%79.3%79.4%79.4%79.4%79.4%79.4%79.5%79.5%79.5%79.5%79.5%79.5%79.6%79.6%79.6%79.6%79.6%79.7%79.7%79.7%79.7%79.7%79.7%79.8%79.8%79.8%79.8%79.8%79.8%79.9%79.9%79.9%79.9%79.9%80.0%80.0%80.0%80.0%80.0%80.0%80.1%80.1%80.1%80.1%80.1%80.2%80.2%80.2%80.2%80.2%80.2%80.3%80.3%80.3%80.3%80.3%80.4%80.4%80.4%80.4%80.4%80.4%80.5%80.5%80.5%80.5%80.5%80.6%80.6%80.6%80.6%80.6%80.6%80.7%80.7%80.7%80.7%80.7%80.7%80.8%80.8%80.8%80.8%80.8%80.9%80.9%80.9%80.9%80.9%80.9%81.0%81.0%81.0%81.0%81.0%81.1%81.1%81.1%81.1%81.1%81.1%81.2%81.2%81.2%81.2%81.2%81.3%81.3%81.3%81.3%81.3%81.3%81.4%81.4%81.4%81.4%81.4%81.5%81.5%81.5%81.5%81.5%81.5%81.6%81.6%81.6%81.6%81.6%81.6%81.7%81.7%81.7%81.7%81.7%81.8%81.8%81.8%81.8%81.8%81.8%81.9%81.9%81.9%81.9%81.9%82.0%82.0%82.0%82.0%82.0%82.0%82.1%82.1%82.1%82.1%82.1%82.2%82.2%82.2%82.2%82.2%82.2%82.3%82.3%82.3%82.3%82.3%82.4%82.4%82.4%82.4%82.4%82.4%82.5%82.5%82.5%82.5%82.5%82.5%82.6%82.6%82.6%82.6%82.6%82.7%82.7%82.7%82.7%82.7%82.7%82.8%82.8%82.8%82.8%82.8%82.9%82.9%82.9%82.9%82.9%82.9%83.0%83.0%83.0%83.0%83.0%83.1%83.1%83.1%83.1%83.1%83.1%83.2%83.2%83.2%83.2%83.2%83.3%83.3%83.3%83.3%83.3%83.3%83.4%83.4%83.4%83.4%83.4%83.4%83.5%83.5%83.5%83.5%83.5%83.6%83.6%83.6%83.6%83.6%83.6%83.7%83.7%83.7%83.7%83.7%83.8%83.8%83.8%83.8%83.8%83.8%83.9%83.9%83.9%83.9%83.9%84.0%84.0%84.0%84.0%84.0%84.0%84.1%84.1%84.1%84.1%84.1%84.2%84.2%84.2%84.2%84.2%84.2%84.3%84.3%84.3%84.3%84.3%84.3%84.4%84.4%84.4%84.4%84.4%84.5%84.5%84.5%84.5%84.5%84.5%84.6%84.6%84.6%84.6%84.6%84.7%84.7%84.7%84.7%84.7%84.7%84.8%84.8%84.8%84.8%84.8%84.9%84.9%84.9%84.9%84.9%84.9%85.0%85.0%85.0%85.0%85.0%85.1%85.1%85.1%85.1%85.1%85.1%85.2%85.2%85.2%85.2%85.2%85.2%85.3%85.3%85.3%85.3%85.3%85.4%85.4%85.4%85.4%85.4%85.4%85.5%85.5%85.5%85.5%85.5%85.6%85.6%85.6%85.6%85.6%85.6%85.7%85.7%85.7%85.7%85.7%85.8%85.8%85.8%85.8%85.8%85.8%85.9%85.9%85.9%85.9%85.9%86.0%86.0%86.0%86.0%86.0%86.0%86.1%86.1%86.1%86.1%86.1%86.1%86.2%86.2%86.2%86.2%86.2%86.3%86.3%86.3%86.3%86.3%86.3%86.4%86.4%86.4%86.4%86.4%86.5%86.5%86.5%86.5%86.5%86.5%86.6%86.6%86.6%86.6%86.6%86.7%86.7%86.7%86.7%86.7%86.7%86.8%86.8%86.8%86.8%86.8%86.9%86.9%86.9%86.9%86.9%86.9%87.0%87.0%87.0%87.0%87.0%87.0%87.1%87.1%87.1%87.1%87.1%87.2%87.2%87.2%87.2%87.2%87.2%87.3%87.3%87.3%87.3%87.3%87.4%87.4%87.4%87.4%87.4%87.4%87.5%87.5%87.5%87.5%87.5%87.6%87.6%87.6%87.6%87.6%87.6%87.7%87.7%87.7%87.7%87.7%87.8%87.8%87.8%87.8%87.8%87.8%87.9%87.9%87.9%87.9%87.9%87.9%88.0%88.0%88.0%88.0%88.0%88.1%88.1%88.1%88.1%88.1%88.1%88.2%88.2%88.2%88.2%88.2%88.3%88.3%88.3%88.3%88.3%88.3%88.4%88.4%88.4%88.4%88.4%88.5%88.5%88.5%88.5%88.5%88.5%88.6%88.6%88.6%88.6%88.6%88.7%88.7%88.7%88.7%88.7%88.7%88.8%88.8%88.8%88.8%88.8%88.8%88.9%88.9%88.9%88.9%88.9%89.0%89.0%89.0%89.0%89.0%89.0%89.1%89.1%89.1%89.1%89.1%89.2%89.2%89.2%89.2%89.2%89.2%89.3%89.3%89.3%89.3%89.3%89.4%89.4%89.4%89.4%89.4%89.4%89.5%89.5%89.5%89.5%89.5%89.6%89.6%89.6%89.6%89.6%89.6%89.7%89.7%89.7%89.7%89.7%89.7%89.8%89.8%89.8%89.8%89.8%89.9%89.9%89.9%89.9%89.9%89.9%90.0%90.0%90.0%90.0%90.0%90.1%90.1%90.1%90.1%90.1%90.1%90.2%90.2%90.2%90.2%90.2%90.3%90.3%90.3%90.3%90.3%90.3%90.4%90.4%90.4%90.4%90.4%90.5%90.5%90.5%90.5%90.5%90.5%90.6%90.6%90.6%90.6%90.6%90.6%90.7%90.7%90.7%90.7%90.7%90.8%90.8%90.8%90.8%90.8%90.8%90.9%90.9%90.9%90.9%90.9%91.0%91.0%91.0%91.0%91.0%91.0%91.1%91.1%91.1%91.1%91.1%91.2%91.2%91.2%91.2%91.2%91.2%91.3%91.3%91.3%91.3%91.3%91.4%91.4%91.4%91.4%91.4%91.4%91.5%91.5%91.5%91.5%91.5%91.5%91.6%91.6%91.6%91.6%91.6%91.7%91.7%91.7%91.7%91.7%91.7%91.8%91.8%91.8%91.8%91.8%91.9%91.9%91.9%91.9%91.9%91.9%92.0%92.0%92.0%92.0%92.0%92.1%92.1%92.1%92.1%92.1%92.1%92.2%92.2%92.2%92.2%92.2%92.3%92.3%92.3%92.3%92.3%92.3%92.4%92.4%92.4%92.4%92.4%92.4%92.5%92.5%92.5%92.5%92.5%92.6%92.6%92.6%92.6%92.6%92.6%92.7%92.7%92.7%92.7%92.7%92.8%92.8%92.8%92.8%92.8%92.8%92.9%92.9%92.9%92.9%92.9%93.0%93.0%93.0%93.0%93.0%93.0%93.1%93.1%93.1%93.1%93.1%93.2%93.2%93.2%93.2%93.2%93.2%93.3%93.3%93.3%93.3%93.3%93.3%93.4%93.4%93.4%93.4%93.4%93.5%93.5%93.5%93.5%93.5%93.5%93.6%93.6%93.6%93.6%93.6%93.7%93.7%93.7%93.7%93.7%93.7%93.8%93.8%93.8%93.8%93.8%93.9%93.9%93.9%93.9%93.9%93.9%94.0%94.0%94.0%94.0%94.0%94.1%94.1%94.1%94.1%94.1%94.1%94.2%94.2%94.2%94.2%94.2%94.2%94.3%94.3%94.3%94.3%94.3%94.4%94.4%94.4%94.4%94.4%94.4%94.5%94.5%94.5%94.5%94.5%94.6%94.6%94.6%94.6%94.6%94.6%94.7%94.7%94.7%94.7%94.7%94.8%94.8%94.8%94.8%94.8%94.8%94.9%94.9%94.9%94.9%94.9%95.0%95.0%95.0%95.0%95.0%95.0%95.1%95.1%95.1%95.1%95.1%95.1%95.2%95.2%95.2%95.2%95.2%95.3%95.3%95.3%95.3%95.3%95.3%95.4%95.4%95.4%95.4%95.4%95.5%95.5%95.5%95.5%95.5%95.5%95.6%95.6%95.6%95.6%95.6%95.7%95.7%95.7%95.7%95.7%95.7%95.8%95.8%95.8%95.8%95.8%95.9%95.9%95.9%95.9%95.9%95.9%96.0%96.0%96.0%96.0%96.0%96.0%96.1%96.1%96.1%96.1%96.1%96.2%96.2%96.2%96.2%96.2%96.2%96.3%96.3%96.3%96.3%96.3%96.4%96.4%96.4%96.4%96.4%96.4%96.5%96.5%96.5%96.5%96.5%96.6%96.6%96.6%96.6%96.6%96.6%96.7%96.7%96.7%96.7%96.7%96.8%96.8%96.8%96.8%96.8%96.8%96.9%96.9%96.9%96.9%96.9%96.9%97.0%97.0%97.0%97.0%97.0%97.1%97.1%97.1%97.1%97.1%97.1%97.2%97.2%97.2%97.2%97.2%97.3%97.3%97.3%97.3%97.3%97.3%97.4%97.4%97.4%97.4%97.4%97.5%97.5%97.5%97.5%97.5%97.5%97.6%97.6%97.6%97.6%97.6%97.7%97.7%97.7%97.7%97.7%97.7%97.8%97.8%97.8%97.8%97.8%97.8%97.9%97.9%97.9%97.9%97.9%98.0%98.0%98.0%98.0%98.0%98.0%98.1%98.1%98.1%98.1%98.1%98.2%98.2%98.2%98.2%98.2%98.2%98.3%98.3%98.3%98.3%98.3%98.4%98.4%98.4%98.4%98.4%98.4%98.5%98.5%98.5%98.5%98.5%98.6%98.6%98.6%98.6%98.6%98.6%98.7%98.7%98.7%98.7%98.7%98.7%98.8%98.8%98.8%98.8%98.8%98.9%98.9%98.9%98.9%98.9%98.9%99.0%99.0%99.0%99.0%99.0%99.1%99.1%99.1%99.1%99.1%99.1%99.2%99.2%99.2%99.2%99.2%99.3%99.3%99.3%99.3%99.3%99.3%99.4%99.4%99.4%99.4%99.4%99.5%99.5%99.5%99.5%99.5%99.5%99.6%99.6%99.6%99.6%99.6%99.6%99.7%99.7%99.7%99.7%99.7%99.8%99.8%99.8%99.8%99.8%99.8%99.9%99.9%99.9%99.9%99.9%100.0%100.0%100.0%100.0%
0.1%0.1%0.2%0.2%0.3%0.3%0.4%0.4%0.5%0.5%0.6%0.6%0.7%0.7%0.8%0.8%0.9%0.9%1.0%1.0%1.1%1.1%1.2%1.2%1.3%1.3%1.4%1.4%1.5%1.5%1.6%1.6%1.7%1.7%1.8%1.8%1.9%1.9%2.0%2.0%2.1%2.1%2.2%2.2%2.3%2.3%2.4%2.4%2.5%2.5%2.6%2.7%2.7%2.8%2.8%2.9%2.9%3.0%3.0%3.1%3.1%3.2%3.2%3.3%3.3%3.4%3.4%3.5%3.5%3.6%3.6%3.7%3.7%3.8%3.8%3.9%3.9%4.0%4.0%4.1%4.1%4.2%4.2%4.3%4.3%4.4%4.4%4.5%4.5%4.6%4.6%4.7%4.7%4.8%4.8%4.9%4.9%5.0%5.0%5.1%5.1%5.2%5.3%5.3%5.4%5.4%5.5%5.5%5.6%5.6%5.7%5.7%5.8%5.8%5.9%5.9%6.0%6.0%6.1%6.1%6.2%6.2%6.3%6.3%6.4%6.4%6.5%6.5%6.6%6.6%6.7%6.7%6.8%6.8%6.9%6.9%7.0%7.0%7.1%7.1%7.2%7.2%7.3%7.3%7.4%7.4%7.5%7.5%7.6%7.6%7.7%7.7%7.8%7.9%7.9%8.0%8.0%8.1%8.1%8.2%8.2%8.3%8.3%8.4%8.4%8.5%8.5%8.6%8.6%8.7%8.7%8.8%8.8%8.9%8.9%9.0%9.0%9.1%9.1%9.2%9.2%9.3%9.3%9.4%9.4%9.5%9.5%9.6%9.6%9.7%9.7%9.8%9.8%9.9%9.9%10.0%10.0%10.1%10.1%10.2%10.2%10.3%10.3%10.4%10.5%10.5%10.6%10.6%10.7%10.7%10.8%10.8%10.9%10.9%11.0%11.0%11.1%11.1%11.2%11.2%11.3%11.3%11.4%11.4%11.5%11.5%11.6%11.6%11.7%11.7%11.8%11.8%11.9%11.9%12.0%12.0%12.1%12.1%12.2%12.2%12.3%12.3%12.4%12.4%12.5%12.5%12.6%12.6%12.7%12.7%12.8%12.8%12.9%12.9%13.0%13.1%13.1%13.2%13.2%13.3%13.3%13.4%13.4%13.5%13.5%13.6%13.6%13.7%13.7%13.8%13.8%13.9%13.9%14.0%14.0%14.1%14.1%14.2%14.2%14.3%14.3%14.4%14.4%14.5%14.5%14.6%14.6%14.7%14.7%14.8%14.8%14.9%14.9%15.0%15.0%15.1%15.1%15.2%15.2%15.3%15.3%15.4%15.4%15.5%15.5%15.6%15.7%15.7%15.8%15.8%15.9%15.9%16.0%16.0%16.1%16.1%16.2%16.2%16.3%16.3%16.4%16.4%16.5%16.5%16.6%16.6%16.7%16.7%16.8%16.8%16.9%16.9%17.0%17.0%17.1%17.1%17.2%17.2%17.3%17.3%17.4%17.4%17.5%17.5%17.6%17.6%17.7%17.7%17.8%17.8%17.9%17.9%18.0%18.0%18.1%18.1%18.2%18.3%18.3%18.4%18.4%18.5%18.5%18.6%18.6%18.7%18.7%18.8%18.8%18.9%18.9%19.0%19.0%19.1%19.1%19.2%19.2%19.3%19.3%19.4%19.4%19.5%19.5%19.6%19.6%19.7%19.7%19.8%19.8%19.9%19.9%20.0%20.0%20.1%20.1%20.2%20.2%20.3%20.3%20.4%20.4%20.5%20.5%20.6%20.6%20.7%20.7%20.8%20.9%20.9%21.0%21.0%21.1%21.1%21.2%21.2%21.3%21.3%21.4%21.4%21.5%21.5%21.6%21.6%21.7%21.7%21.8%21.8%21.9%21.9%22.0%22.0%22.1%22.1%22.2%22.2%22.3%22.3%22.4%22.4%22.5%22.5%22.6%22.6%22.7%22.7%22.8%22.8%22.9%22.9%23.0%23.0%23.1%23.1%23.2%23.2%23.3%23.3%23.4%23.5%23.5%23.6%23.6%23.7%23.7%23.8%23.8%23.9%23.9%24.0%24.0%24.1%24.1%24.2%24.2%24.3%24.3%24.4%24.4%24.5%24.5%24.6%24.6%24.7%24.7%24.8%24.8%24.9%24.9%25.0%25.0%25.1%25.1%25.2%25.2%25.3%25.3%25.4%25.4%25.5%25.5%25.6%25.6%25.7%25.7%25.8%25.8%25.9%25.9%26.0%26.1%26.1%26.2%26.2%26.3%26.3%26.4%26.4%26.5%26.5%26.6%26.6%26.7%26.7%26.8%26.8%26.9%26.9%27.0%27.0%27.1%27.1%27.2%27.2%27.3%27.3%27.4%27.4%27.5%27.5%27.6%27.6%27.7%27.7%27.8%27.8%27.9%27.9%28.0%28.0%28.1%28.1%28.2%28.2%28.3%28.3%28.4%28.4%28.5%28.5%28.6%28.7%28.7%28.8%28.8%28.9%28.9%29.0%29.0%29.1%29.1%29.2%29.2%29.3%29.3%29.4%29.4%29.5%29.5%29.6%29.6%29.7%29.7%29.8%29.8%29.9%29.9%30.0%30.0%30.1%30.1%30.2%30.2%30.3%30.3%30.4%30.4%30.5%30.5%30.6%30.6%30.7%30.7%30.8%30.8%30.9%30.9%31.0%31.0%31.1%31.1%31.2%31.3%31.3%31.4%31.4%31.5%31.5%31.6%31.6%31.7%31.7%31.8%31.8%31.9%31.9%32.0%32.0%32.1%32.1%32.2%32.2%32.3%32.3%32.4%32.4%32.5%32.5%32.6%32.6%32.7%32.7%32.8%32.8%32.9%32.9%33.0%33.0%33.1%33.1%33.2%33.2%33.3%33.3%33.4%33.4%33.5%33.5%33.6%33.6%33.7%33.7%33.8%33.9%33.9%34.0%34.0%34.1%34.1%34.2%34.2%34.3%34.3%34.4%34.4%34.5%34.5%34.6%34.6%34.7%34.7%34.8%34.8%34.9%34.9%35.0%35.0%35.1%35.1%35.2%35.2%35.3%35.3%35.4%35.4%35.5%35.5%35.6%35.6%35.7%35.7%35.8%35.8%35.9%35.9%36.0%36.0%36.1%36.1%36.2%36.2%36.3%36.3%36.4%36.5%36.5%36.6%36.6%36.7%36.7%36.8%36.8%36.9%36.9%37.0%37.0%37.1%37.1%37.2%37.2%37.3%37.3%37.4%37.4%37.5%37.5%37.6%37.6%37.7%37.7%37.8%37.8%37.9%37.9%38.0%38.0%38.1%38.1%38.2%38.2%38.3%38.3%38.4%38.4%38.5%38.5%38.6%38.6%38.7%38.7%38.8%38.8%38.9%38.9%39.0%39.1%39.1%39.2%39.2%39.3%39.3%39.4%39.4%39.5%39.5%39.6%39.6%39.7%39.7%39.8%39.8%39.9%39.9%40.0%40.0%40.1%40.1%40.2%40.2%40.3%40.3%40.4%40.4%40.5%40.5%40.6%40.6%40.7%40.7%40.8%40.8%40.9%40.9%41.0%41.0%41.1%41.1%41.2%41.2%41.3%41.3%41.4%41.4%41.5%41.5%41.6%41.7%41.7%41.8%41.8%41.9%41.9%42.0%42.0%42.1%42.1%42.2%42.2%42.3%42.3%42.4%42.4%42.5%42.5%42.6%42.6%42.7%42.7%42.8%42.8%42.9%42.9%43.0%43.0%43.1%43.1%43.2%43.2%43.3%43.3%43.4%43.4%43.5%43.5%43.6%43.6%43.7%43.7%43.8%43.8%43.9%43.9%44.0%44.0%44.1%44.1%44.2%44.3%44.3%44.4%44.4%44.5%44.5%44.6%44.6%44.7%44.7%44.8%44.8%44.9%44.9%45.0%45.0%45.1%45.1%45.2%45.2%45.3%45.3%45.4%45.4%45.5%45.5%45.6%45.6%45.7%45.7%45.8%45.8%45.9%45.9%46.0%46.0%46.1%46.1%46.2%46.2%46.3%46.3%46.4%46.4%46.5%46.5%46.6%46.6%46.7%46.7%46.8%46.9%46.9%47.0%47.0%47.1%47.1%47.2%47.2%47.3%47.3%47.4%47.4%47.5%47.5%47.6%47.6%47.7%47.7%47.8%47.8%47.9%47.9%48.0%48.0%48.1%48.1%48.2%48.2%48.3%48.3%48.4%48.4%48.5%48.5%48.6%48.6%48.7%48.7%48.8%48.8%48.9%48.9%49.0%49.0%49.1%49.1%49.2%49.2%49.3%49.3%49.4%49.5%49.5%49.6%49.6%49.7%49.7%49.8%49.8%49.9%49.9%50.0%50.0%50.1%50.1%50.2%50.2%50.3%50.3%50.4%50.4%50.5%50.5%50.6%50.6%50.7%50.7%50.8%50.8%50.9%50.9%51.0%51.0%51.1%51.1%51.2%51.2%51.3%51.3%51.4%51.4%51.5%51.5%51.6%51.6%51.7%51.7%51.8%51.8%51.9%51.9%52.0%52.1%52.1%52.2%52.2%52.3%52.3%52.4%52.4%52.5%52.5%52.6%52.6%52.7%52.7%52.8%52.8%52.9%52.9%53.0%53.0%53.1%53.1%53.2%53.2%53.3%53.3%53.4%53.4%53.5%53.5%53.6%53.6%53.7%53.7%53.8%53.8%53.9%53.9%54.0%54.0%54.1%54.1%54.2%54.2%54.3%54.3%54.4%54.4%54.5%54.5%54.6%54.7%54.7%54.8%54.8%54.9%54.9%55.0%55.0%55.1%55.1%55.2%55.2%55.3%55.3%55.4%55.4%55.5%55.5%55.6%55.6%55.7%55.7%55.8%55.8%55.9%55.9%56.0%56.0%56.1%56.1%56.2%56.2%56.3%56.3%56.4%56.4%56.5%56.5%56.6%56.6%56.7%56.7%56.8%56.8%56.9%56.9%57.0%57.0%57.1%57.1%57.2%57.3%57.3%57.4%57.4%57.5%57.5%57.6%57.6%57.7%57.7%57.8%57.8%57.9%57.9%58.0%58.0%58.1%58.1%58.2%58.2%58.3%58.3%58.4%58.4%58.5%58.5%58.6%58.6%58.7%58.7%58.8%58.8%58.9%58.9%59.0%59.0%59.1%59.1%59.2%59.2%59.3%59.3%59.4%59.4%59.5%59.5%59.6%59.6%59.7%59.7%59.8%59.9%59.9%60.0%60.0%60.1%60.1%60.2%60.2%60.3%60.3%60.4%60.4%60.5%60.5%60.6%60.6%60.7%60.7%60.8%60.8%60.9%60.9%61.0%61.0%61.1%61.1%61.2%61.2%61.3%61.3%61.4%61.4%61.5%61.5%61.6%61.6%61.7%61.7%61.8%61.8%61.9%61.9%62.0%62.0%62.1%62.1%62.2%62.2%62.3%62.3%62.4%62.5%62.5%62.6%62.6%62.7%62.7%62.8%62.8%62.9%62.9%63.0%63.0%63.1%63.1%63.2%63.2%63.3%63.3%63.4%63.4%63.5%63.5%63.6%63.6%63.7%63.7%63.8%63.8%63.9%63.9%64.0%64.0%64.1%64.1%64.2%64.2%64.3%64.3%64.4%64.4%64.5%64.5%64.6%64.6%64.7%64.7%64.8%64.8%64.9%64.9%65.0%65.1%65.1%65.2%65.2%65.3%65.3%65.4%65.4%65.5%65.5%65.6%65.6%65.7%65.7%65.8%65.8%65.9%65.9%66.0%66.0%66.1%66.1%66.2%66.2%66.3%66.3%66.4%66.4%66.5%66.5%66.6%66.6%66.7%66.7%66.8%66.8%66.9%66.9%67.0%67.0%67.1%67.1%67.2%67.2%67.3%67.3%67.4%67.4%67.5%67.5%67.6%67.7%67.7%67.8%67.8%67.9%67.9%68.0%68.0%68.1%68.1%68.2%68.2%68.3%68.3%68.4%68.4%68.5%68.5%68.6%68.6%68.7%68.7%68.8%68.8%68.9%68.9%69.0%69.0%69.1%69.1%69.2%69.2%69.3%69.3%69.4%69.4%69.5%69.5%69.6%69.6%69.7%69.7%69.8%69.8%69.9%69.9%70.0%70.0%70.1%70.1%70.2%70.3%70.3%70.4%70.4%70.5%70.5%70.6%70.6%70.7%70.7%70.8%70.8%70.9%70.9%71.0%71.0%71.1%71.1%71.2%71.2%71.3%71.3%71.4%71.4%71.5%71.5%71.6%71.6%71.7%71.7%71.8%71.8%71.9%71.9%72.0%72.0%72.1%72.1%72.2%72.2%72.3%72.3%72.4%72.4%72.5%72.5%72.6%72.6%72.7%72.7%72.8%72.9%72.9%73.0%73.0%73.1%73.1%73.2%73.2%73.3%73.3%73.4%73.4%73.5%73.5%73.6%73.6%73.7%73.7%73.8%73.8%73.9%73.9%74.0%74.0%74.1%74.1%74.2%74.2%74.3%74.3%74.4%74.4%74.5%74.5%74.6%74.6%74.7%74.7%74.8%74.8%74.9%74.9%75.0%75.0%75.1%75.1%75.2%75.2%75.3%75.3%75.4%75.5%75.5%75.6%75.6%75.7%75.7%75.8%75.8%75.9%75.9%76.0%76.0%76.1%76.1%76.2%76.2%76.3%76.3%76.4%76.4%76.5%76.5%76.6%76.6%76.7%76.7%76.8%76.8%76.9%76.9%77.0%77.0%77.1%77.1%77.2%77.2%77.3%77.3%77.4%77.4%77.5%77.5%77.6%77.6%77.7%77.7%77.8%77.8%77.9%77.9%78.0%78.1%78.1%78.2%78.2%78.3%78.3%78.4%78.4%78.5%78.5%78.6%78.6%78.7%78.7%78.8%78.8%78.9%78.9%79.0%79.0%79.1%79.1%79.2%79.2%79.3%79.3%79.4%79.4%79.5%79.5%79.6%79.6%79.7%79.7%79.8%79.8%79.9%79.9%80.0%80.0%80.1%80.1%80.2%80.2%80.3%80.3%80.4%80.4%80.5%80.5%80.6%80.7%80.7%80.8%80.8%80.9%80.9%81.0%81.0%81.1%81.1%81.2%81.2%81.3%81.3%81.4%81.4%81.5%81.5%81.6%81.6%81.7%81.7%81.8%81.8%81.9%81.9%82.0%82.0%82.1%82.1%82.2%82.2%82.3%82.3%82.4%82.4%82.5%82.5%82.6%82.6%82.7%82.7%82.8%82.8%82.9%82.9%83.0%83.0%83.1%83.1%83.2%83.3%83.3%83.4%83.4%83.5%83.5%83.6%83.6%83.7%83.7%83.8%83.8%83.9%83.9%84.0%84.0%84.1%84.1%84.2%84.2%84.3%84.3%84.4%84.4%84.5%84.5%84.6%84.6%84.7%84.7%84.8%84.8%84.9%84.9%85.0%85.0%85.1%85.1%85.2%85.2%85.3%85.3%85.4%85.4%85.5%85.5%85.6%85.6%85.7%85.7%85.8%85.9%85.9%86.0%86.0%86.1%86.1%86.2%86.2%86.3%86.3%86.4%86.4%86.5%86.5%86.6%86.6%86.7%86.7%86.8%86.8%86.9%86.9%87.0%87.0%87.1%87.1%87.2%87.2%87.3%87.3%87.4%87.4%87.5%87.5%87.6%87.6%87.7%87.7%87.8%87.8%87.9%87.9%88.0%88.0%88.1%88.1%88.2%88.2%88.3%88.3%88.4%88.5%88.5%88.6%88.6%88.7%88.7%88.8%88.8%88.9%88.9%89.0%89.0%89.1%89.1%89.2%89.2%89.3%89.3%89.4%89.4%89.5%89.5%89.6%89.6%89.7%89.7%89.8%89.8%89.9%89.9%90.0%90.0%90.1%90.1%90.2%90.2%90.3%90.3%90.4%90.4%90.5%90.5%90.6%90.6%90.7%90.7%90.8%90.8%90.9%90.9%91.0%91.1%91.1%91.2%91.2%91.3%91.3%91.4%91.4%91.5%91.5%91.6%91.6%91.7%91.7%91.8%91.8%91.9%91.9%92.0%92.0%92.1%92.1%92.2%92.2%92.3%92.3%92.4%92.4%92.5%92.5%92.6%92.6%92.7%92.7%92.8%92.8%92.9%92.9%93.0%93.0%93.1%93.1%93.2%93.2%93.3%93.3%93.4%93.4%93.5%93.5%93.6%93.7%93.7%93.8%93.8%93.9%93.9%94.0%94.0%94.1%94.1%94.2%94.2%94.3%94.3%94.4%94.4%94.5%94.5%94.6%94.6%94.7%94.7%94.8%94.8%94.9%94.9%95.0%95.0%95.1%95.1%95.2%95.2%95.3%95.3%95.4%95.4%95.5%95.5%95.6%95.6%95.7%95.7%95.8%95.8%95.9%95.9%96.0%96.0%96.1%96.1%96.2%96.3%96.3%96.4%96.4%96.5%96.5%96.6%96.6%96.7%96.7%96.8%96.8%96.9%96.9%97.0%97.0%97.1%97.1%97.2%97.2%97.3%97.3%97.4%97.4%97.5%97.5%97.6%97.6%97.7%97.7%97.8%97.8%97.9%97.9%98.0%98.0%98.1%98.1%98.2%98.2%98.3%98.3%98.4%98.4%98.5%98.5%98.6%98.6%98.7%98.7%98.8%98.9%98.9%99.0%99.0%99.1%99.1%99.2%99.2%99.3%99.3%99.4%99.4%99.5%99.5%99.6%99.6%99.7%99.7%99.8%99.8%99.9%99.9%100.0%100.0%
Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/SVHN/color/raw/train/train_32x32.mat
Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/SVHN/color/raw/test/test_32x32.mat
Train - Iteration 0: 	Loss_clf1: 0.0167, Loss_clf2: 0.0166, Loss_discrepancy: 0.0007
Train - Iteration 50: 	Loss_clf1: 0.1099, Loss_clf2: 0.1112, Loss_discrepancy: 0.0136
Train - Iteration 100: 	Loss_clf1: 0.0697, Loss_clf2: 0.0696, Loss_discrepancy: 0.0113
Train - Iteration 150: 	Loss_clf1: 0.0597, Loss_clf2: 0.0578, Loss_discrepancy: 0.0115
Train - Iteration 200: 	Loss_clf1: 0.0580, Loss_clf2: 0.0566, Loss_discrepancy: 0.0118
Train - Iteration 250: 	Loss_clf1: 0.0497, Loss_clf2: 0.0499, Loss_discrepancy: 0.0131
Train - Iteration 300: 	Loss_clf1: 0.0492, Loss_clf2: 0.0488, Loss_discrepancy: 0.0133
Train - Iteration 350: 	Loss_clf1: 0.0494, Loss_clf2: 0.0466, Loss_discrepancy: 0.0123
Train - Iteration 400: 	Loss_clf1: 0.0575, Loss_clf2: 0.0561, Loss_discrepancy: 0.0120
Train - Iteration 450: 	Loss_clf1: 0.0483, Loss_clf2: 0.0454, Loss_discrepancy: 0.0124
Train - Epoch [1]: 		Loss_clf1: 0.0620, Loss_clf2: 0.0611, Loss_discrepancy: 0.0124
Test - Epoch [1]: Accuracy_clf1: 11.64%, Accuracy_clf2: 11.64%, Accuracy_ensemble: 11.57%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0005, Loss_clf2: 0.0004, Loss_discrepancy: 0.0002
Train - Iteration 50: 	Loss_clf1: 0.0448, Loss_clf2: 0.0422, Loss_discrepancy: 0.0109
Train - Iteration 100: 	Loss_clf1: 0.0372, Loss_clf2: 0.0388, Loss_discrepancy: 0.0096
Train - Iteration 150: 	Loss_clf1: 0.0434, Loss_clf2: 0.0449, Loss_discrepancy: 0.0118
Train - Iteration 200: 	Loss_clf1: 0.0421, Loss_clf2: 0.0427, Loss_discrepancy: 0.0107
Train - Iteration 250: 	Loss_clf1: 0.0390, Loss_clf2: 0.0389, Loss_discrepancy: 0.0105
Train - Iteration 300: 	Loss_clf1: 0.0459, Loss_clf2: 0.0454, Loss_discrepancy: 0.0097
Train - Iteration 350: 	Loss_clf1: 0.0421, Loss_clf2: 0.0417, Loss_discrepancy: 0.0099
Train - Iteration 400: 	Loss_clf1: 0.0473, Loss_clf2: 0.0470, Loss_discrepancy: 0.0103
Train - Iteration 450: 	Loss_clf1: 0.0369, Loss_clf2: 0.0367, Loss_discrepancy: 0.0092
Train - Epoch [2]: 		Loss_clf1: 0.0424, Loss_clf2: 0.0423, Loss_discrepancy: 0.0103
Test - Epoch [2]: Accuracy_clf1: 11.96%, Accuracy_clf2: 11.92%, Accuracy_ensemble: 11.97%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0017, Loss_clf2: 0.0016, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0342, Loss_clf2: 0.0356, Loss_discrepancy: 0.0078
Train - Iteration 100: 	Loss_clf1: 0.0378, Loss_clf2: 0.0363, Loss_discrepancy: 0.0079
Train - Iteration 150: 	Loss_clf1: 0.0392, Loss_clf2: 0.0402, Loss_discrepancy: 0.0081
Train - Iteration 200: 	Loss_clf1: 0.0310, Loss_clf2: 0.0318, Loss_discrepancy: 0.0084
Train - Iteration 250: 	Loss_clf1: 0.0362, Loss_clf2: 0.0365, Loss_discrepancy: 0.0095
Train - Iteration 300: 	Loss_clf1: 0.0407, Loss_clf2: 0.0391, Loss_discrepancy: 0.0094
Train - Iteration 350: 	Loss_clf1: 0.0384, Loss_clf2: 0.0379, Loss_discrepancy: 0.0095
Train - Iteration 400: 	Loss_clf1: 0.0315, Loss_clf2: 0.0307, Loss_discrepancy: 0.0093
Train - Iteration 450: 	Loss_clf1: 0.0376, Loss_clf2: 0.0362, Loss_discrepancy: 0.0093
Train - Epoch [3]: 		Loss_clf1: 0.0361, Loss_clf2: 0.0359, Loss_discrepancy: 0.0087
Test - Epoch [3]: Accuracy_clf1: 10.25%, Accuracy_clf2: 10.20%, Accuracy_ensemble: 10.23%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0010, Loss_clf2: 0.0011, Loss_discrepancy: 0.0002
Train - Iteration 50: 	Loss_clf1: 0.0372, Loss_clf2: 0.0367, Loss_discrepancy: 0.0077
Train - Iteration 100: 	Loss_clf1: 0.0309, Loss_clf2: 0.0315, Loss_discrepancy: 0.0083
Train - Iteration 150: 	Loss_clf1: 0.0417, Loss_clf2: 0.0430, Loss_discrepancy: 0.0075
Train - Iteration 200: 	Loss_clf1: 0.0325, Loss_clf2: 0.0326, Loss_discrepancy: 0.0088
Train - Iteration 250: 	Loss_clf1: 0.0343, Loss_clf2: 0.0344, Loss_discrepancy: 0.0111
Train - Iteration 300: 	Loss_clf1: 0.0371, Loss_clf2: 0.0353, Loss_discrepancy: 0.0068
Train - Iteration 350: 	Loss_clf1: 0.0310, Loss_clf2: 0.0313, Loss_discrepancy: 0.0067
Train - Iteration 400: 	Loss_clf1: 0.0394, Loss_clf2: 0.0406, Loss_discrepancy: 0.0070
Train - Iteration 450: 	Loss_clf1: 0.0327, Loss_clf2: 0.0311, Loss_discrepancy: 0.0066
Train - Epoch [4]: 		Loss_clf1: 0.0352, Loss_clf2: 0.0352, Loss_discrepancy: 0.0078
Test - Epoch [4]: Accuracy_clf1: 10.68%, Accuracy_clf2: 10.83%, Accuracy_ensemble: 10.77%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0003, Loss_clf2: 0.0003, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0342, Loss_clf2: 0.0335, Loss_discrepancy: 0.0069
Train - Iteration 100: 	Loss_clf1: 0.0325, Loss_clf2: 0.0331, Loss_discrepancy: 0.0071
Train - Iteration 150: 	Loss_clf1: 0.0301, Loss_clf2: 0.0294, Loss_discrepancy: 0.0065
Train - Iteration 200: 	Loss_clf1: 0.0348, Loss_clf2: 0.0350, Loss_discrepancy: 0.0063
Train - Iteration 250: 	Loss_clf1: 0.0241, Loss_clf2: 0.0251, Loss_discrepancy: 0.0063
Train - Iteration 300: 	Loss_clf1: 0.0363, Loss_clf2: 0.0361, Loss_discrepancy: 0.0071
Train - Iteration 350: 	Loss_clf1: 0.0394, Loss_clf2: 0.0392, Loss_discrepancy: 0.0067
Train - Iteration 400: 	Loss_clf1: 0.0303, Loss_clf2: 0.0312, Loss_discrepancy: 0.0068
Train - Iteration 450: 	Loss_clf1: 0.0293, Loss_clf2: 0.0286, Loss_discrepancy: 0.0063
Train - Epoch [5]: 		Loss_clf1: 0.0321, Loss_clf2: 0.0321, Loss_discrepancy: 0.0067
Test - Epoch [5]: Accuracy_clf1: 10.84%, Accuracy_clf2: 10.99%, Accuracy_ensemble: 10.88%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0008, Loss_clf2: 0.0011, Loss_discrepancy: 0.0002
Train - Iteration 50: 	Loss_clf1: 0.0242, Loss_clf2: 0.0252, Loss_discrepancy: 0.0069
Train - Iteration 100: 	Loss_clf1: 0.0251, Loss_clf2: 0.0256, Loss_discrepancy: 0.0064
Train - Iteration 150: 	Loss_clf1: 0.0306, Loss_clf2: 0.0296, Loss_discrepancy: 0.0065
Train - Iteration 200: 	Loss_clf1: 0.0330, Loss_clf2: 0.0331, Loss_discrepancy: 0.0065
Train - Iteration 250: 	Loss_clf1: 0.0320, Loss_clf2: 0.0324, Loss_discrepancy: 0.0057
Train - Iteration 300: 	Loss_clf1: 0.0305, Loss_clf2: 0.0306, Loss_discrepancy: 0.0055
Train - Iteration 350: 	Loss_clf1: 0.0342, Loss_clf2: 0.0342, Loss_discrepancy: 0.0062
Train - Iteration 400: 	Loss_clf1: 0.0256, Loss_clf2: 0.0260, Loss_discrepancy: 0.0060
Train - Iteration 450: 	Loss_clf1: 0.0319, Loss_clf2: 0.0309, Loss_discrepancy: 0.0066
Train - Epoch [6]: 		Loss_clf1: 0.0298, Loss_clf2: 0.0299, Loss_discrepancy: 0.0063
Test - Epoch [6]: Accuracy_clf1: 11.00%, Accuracy_clf2: 11.09%, Accuracy_ensemble: 11.05%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0013, Loss_clf2: 0.0013, Loss_discrepancy: 0.0002
Train - Iteration 50: 	Loss_clf1: 0.0297, Loss_clf2: 0.0290, Loss_discrepancy: 0.0066
Train - Iteration 100: 	Loss_clf1: 0.0275, Loss_clf2: 0.0268, Loss_discrepancy: 0.0068
Train - Iteration 150: 	Loss_clf1: 0.0257, Loss_clf2: 0.0262, Loss_discrepancy: 0.0063
Train - Iteration 200: 	Loss_clf1: 0.0244, Loss_clf2: 0.0238, Loss_discrepancy: 0.0069
Train - Iteration 250: 	Loss_clf1: 0.0273, Loss_clf2: 0.0274, Loss_discrepancy: 0.0064
Train - Iteration 300: 	Loss_clf1: 0.0301, Loss_clf2: 0.0299, Loss_discrepancy: 0.0063
Train - Iteration 350: 	Loss_clf1: 0.0250, Loss_clf2: 0.0249, Loss_discrepancy: 0.0061
Train - Iteration 400: 	Loss_clf1: 0.0284, Loss_clf2: 0.0286, Loss_discrepancy: 0.0065
Train - Iteration 450: 	Loss_clf1: 0.0302, Loss_clf2: 0.0301, Loss_discrepancy: 0.0067
Train - Epoch [7]: 		Loss_clf1: 0.0277, Loss_clf2: 0.0275, Loss_discrepancy: 0.0065
Test - Epoch [7]: Accuracy_clf1: 11.23%, Accuracy_clf2: 11.26%, Accuracy_ensemble: 11.24%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0268, Loss_clf2: 0.0276, Loss_discrepancy: 0.0066
Train - Iteration 100: 	Loss_clf1: 0.0233, Loss_clf2: 0.0239, Loss_discrepancy: 0.0069
Train - Iteration 150: 	Loss_clf1: 0.0248, Loss_clf2: 0.0249, Loss_discrepancy: 0.0057
Train - Iteration 200: 	Loss_clf1: 0.0240, Loss_clf2: 0.0257, Loss_discrepancy: 0.0060
Train - Iteration 250: 	Loss_clf1: 0.0358, Loss_clf2: 0.0357, Loss_discrepancy: 0.0059
Train - Iteration 300: 	Loss_clf1: 0.0242, Loss_clf2: 0.0253, Loss_discrepancy: 0.0061
Train - Iteration 350: 	Loss_clf1: 0.0181, Loss_clf2: 0.0177, Loss_discrepancy: 0.0060
Train - Iteration 400: 	Loss_clf1: 0.0362, Loss_clf2: 0.0345, Loss_discrepancy: 0.0066
Train - Iteration 450: 	Loss_clf1: 0.0251, Loss_clf2: 0.0260, Loss_discrepancy: 0.0060
Train - Epoch [8]: 		Loss_clf1: 0.0264, Loss_clf2: 0.0267, Loss_discrepancy: 0.0062
Test - Epoch [8]: Accuracy_clf1: 11.34%, Accuracy_clf2: 11.28%, Accuracy_ensemble: 11.32%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0003, Loss_clf2: 0.0003, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0217, Loss_clf2: 0.0224, Loss_discrepancy: 0.0061
Train - Iteration 100: 	Loss_clf1: 0.0252, Loss_clf2: 0.0266, Loss_discrepancy: 0.0061
Train - Iteration 150: 	Loss_clf1: 0.0242, Loss_clf2: 0.0249, Loss_discrepancy: 0.0061
Train - Iteration 200: 	Loss_clf1: 0.0280, Loss_clf2: 0.0288, Loss_discrepancy: 0.0061
Train - Iteration 250: 	Loss_clf1: 0.0282, Loss_clf2: 0.0293, Loss_discrepancy: 0.0065
Train - Iteration 300: 	Loss_clf1: 0.0371, Loss_clf2: 0.0367, Loss_discrepancy: 0.0067
Train - Iteration 350: 	Loss_clf1: 0.0327, Loss_clf2: 0.0317, Loss_discrepancy: 0.0057
Train - Iteration 400: 	Loss_clf1: 0.0304, Loss_clf2: 0.0300, Loss_discrepancy: 0.0058
Train - Iteration 450: 	Loss_clf1: 0.0191, Loss_clf2: 0.0211, Loss_discrepancy: 0.0056
Train - Epoch [9]: 		Loss_clf1: 0.0271, Loss_clf2: 0.0277, Loss_discrepancy: 0.0061
Test - Epoch [9]: Accuracy_clf1: 10.58%, Accuracy_clf2: 10.58%, Accuracy_ensemble: 10.58%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0002, Loss_clf2: 0.0002, Loss_discrepancy: 0.0002
Train - Iteration 50: 	Loss_clf1: 0.0201, Loss_clf2: 0.0216, Loss_discrepancy: 0.0066
Train - Iteration 100: 	Loss_clf1: 0.0235, Loss_clf2: 0.0232, Loss_discrepancy: 0.0065
Train - Iteration 150: 	Loss_clf1: 0.0184, Loss_clf2: 0.0184, Loss_discrepancy: 0.0059
Train - Iteration 200: 	Loss_clf1: 0.0219, Loss_clf2: 0.0228, Loss_discrepancy: 0.0074
Train - Iteration 250: 	Loss_clf1: 0.0279, Loss_clf2: 0.0289, Loss_discrepancy: 0.0067
Train - Iteration 300: 	Loss_clf1: 0.0298, Loss_clf2: 0.0280, Loss_discrepancy: 0.0063
Train - Iteration 350: 	Loss_clf1: 0.0239, Loss_clf2: 0.0260, Loss_discrepancy: 0.0060
Train - Iteration 400: 	Loss_clf1: 0.0227, Loss_clf2: 0.0220, Loss_discrepancy: 0.0067
Train - Iteration 450: 	Loss_clf1: 0.0244, Loss_clf2: 0.0244, Loss_discrepancy: 0.0057
Train - Epoch [10]: 		Loss_clf1: 0.0236, Loss_clf2: 0.0239, Loss_discrepancy: 0.0064
Test - Epoch [10]: Accuracy_clf1: 10.74%, Accuracy_clf2: 10.76%, Accuracy_ensemble: 10.76%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0005, Loss_clf2: 0.0006, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0200, Loss_clf2: 0.0203, Loss_discrepancy: 0.0051
Train - Iteration 100: 	Loss_clf1: 0.0168, Loss_clf2: 0.0180, Loss_discrepancy: 0.0051
Train - Iteration 150: 	Loss_clf1: 0.0231, Loss_clf2: 0.0236, Loss_discrepancy: 0.0053
Train - Iteration 200: 	Loss_clf1: 0.0238, Loss_clf2: 0.0232, Loss_discrepancy: 0.0052
Train - Iteration 250: 	Loss_clf1: 0.0280, Loss_clf2: 0.0287, Loss_discrepancy: 0.0053
Train - Iteration 300: 	Loss_clf1: 0.0198, Loss_clf2: 0.0197, Loss_discrepancy: 0.0051
Train - Iteration 350: 	Loss_clf1: 0.0244, Loss_clf2: 0.0244, Loss_discrepancy: 0.0055
Train - Iteration 400: 	Loss_clf1: 0.0283, Loss_clf2: 0.0276, Loss_discrepancy: 0.0056
Train - Iteration 450: 	Loss_clf1: 0.0197, Loss_clf2: 0.0213, Loss_discrepancy: 0.0050
Train - Epoch [11]: 		Loss_clf1: 0.0230, Loss_clf2: 0.0233, Loss_discrepancy: 0.0053
Test - Epoch [11]: Accuracy_clf1: 10.92%, Accuracy_clf2: 10.92%, Accuracy_ensemble: 10.91%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0214, Loss_clf2: 0.0213, Loss_discrepancy: 0.0048
Train - Iteration 100: 	Loss_clf1: 0.0206, Loss_clf2: 0.0197, Loss_discrepancy: 0.0051
Train - Iteration 150: 	Loss_clf1: 0.0230, Loss_clf2: 0.0240, Loss_discrepancy: 0.0053
Train - Iteration 200: 	Loss_clf1: 0.0255, Loss_clf2: 0.0258, Loss_discrepancy: 0.0049
Train - Iteration 250: 	Loss_clf1: 0.0188, Loss_clf2: 0.0201, Loss_discrepancy: 0.0048
Train - Iteration 300: 	Loss_clf1: 0.0149, Loss_clf2: 0.0144, Loss_discrepancy: 0.0046
Train - Iteration 350: 	Loss_clf1: 0.0192, Loss_clf2: 0.0182, Loss_discrepancy: 0.0049
Train - Iteration 400: 	Loss_clf1: 0.0198, Loss_clf2: 0.0197, Loss_discrepancy: 0.0049
Train - Iteration 450: 	Loss_clf1: 0.0266, Loss_clf2: 0.0280, Loss_discrepancy: 0.0054
Train - Epoch [12]: 		Loss_clf1: 0.0214, Loss_clf2: 0.0215, Loss_discrepancy: 0.0050
Test - Epoch [12]: Accuracy_clf1: 10.96%, Accuracy_clf2: 10.83%, Accuracy_ensemble: 10.91%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0002, Loss_clf2: 0.0002, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0166, Loss_clf2: 0.0177, Loss_discrepancy: 0.0050
Train - Iteration 100: 	Loss_clf1: 0.0209, Loss_clf2: 0.0200, Loss_discrepancy: 0.0052
Train - Iteration 150: 	Loss_clf1: 0.0158, Loss_clf2: 0.0167, Loss_discrepancy: 0.0045
Train - Iteration 200: 	Loss_clf1: 0.0125, Loss_clf2: 0.0124, Loss_discrepancy: 0.0047
Train - Iteration 250: 	Loss_clf1: 0.0225, Loss_clf2: 0.0228, Loss_discrepancy: 0.0051
Train - Iteration 300: 	Loss_clf1: 0.0252, Loss_clf2: 0.0250, Loss_discrepancy: 0.0054
Train - Iteration 350: 	Loss_clf1: 0.0237, Loss_clf2: 0.0224, Loss_discrepancy: 0.0049
Train - Iteration 400: 	Loss_clf1: 0.0188, Loss_clf2: 0.0182, Loss_discrepancy: 0.0048
Train - Iteration 450: 	Loss_clf1: 0.0206, Loss_clf2: 0.0209, Loss_discrepancy: 0.0046
Train - Epoch [13]: 		Loss_clf1: 0.0199, Loss_clf2: 0.0199, Loss_discrepancy: 0.0049
Test - Epoch [13]: Accuracy_clf1: 10.76%, Accuracy_clf2: 10.65%, Accuracy_ensemble: 10.66%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0002, Loss_clf2: 0.0004, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0159, Loss_clf2: 0.0155, Loss_discrepancy: 0.0049
Train - Iteration 100: 	Loss_clf1: 0.0138, Loss_clf2: 0.0139, Loss_discrepancy: 0.0049
Train - Iteration 150: 	Loss_clf1: 0.0167, Loss_clf2: 0.0156, Loss_discrepancy: 0.0049
Train - Iteration 200: 	Loss_clf1: 0.0256, Loss_clf2: 0.0248, Loss_discrepancy: 0.0051
Train - Iteration 250: 	Loss_clf1: 0.0198, Loss_clf2: 0.0199, Loss_discrepancy: 0.0049
Train - Iteration 300: 	Loss_clf1: 0.0187, Loss_clf2: 0.0192, Loss_discrepancy: 0.0046
Train - Iteration 350: 	Loss_clf1: 0.0170, Loss_clf2: 0.0164, Loss_discrepancy: 0.0045
Train - Iteration 400: 	Loss_clf1: 0.0213, Loss_clf2: 0.0223, Loss_discrepancy: 0.0048
Train - Iteration 450: 	Loss_clf1: 0.0219, Loss_clf2: 0.0223, Loss_discrepancy: 0.0049
Train - Epoch [14]: 		Loss_clf1: 0.0190, Loss_clf2: 0.0189, Loss_discrepancy: 0.0048
Test - Epoch [14]: Accuracy_clf1: 11.22%, Accuracy_clf2: 11.27%, Accuracy_ensemble: 11.27%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0004, Loss_clf2: 0.0004, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0159, Loss_clf2: 0.0151, Loss_discrepancy: 0.0046
Train - Iteration 100: 	Loss_clf1: 0.0163, Loss_clf2: 0.0161, Loss_discrepancy: 0.0048
Train - Iteration 150: 	Loss_clf1: 0.0201, Loss_clf2: 0.0203, Loss_discrepancy: 0.0049
Train - Iteration 200: 	Loss_clf1: 0.0164, Loss_clf2: 0.0164, Loss_discrepancy: 0.0046
Train - Iteration 250: 	Loss_clf1: 0.0174, Loss_clf2: 0.0177, Loss_discrepancy: 0.0047
Train - Iteration 300: 	Loss_clf1: 0.0182, Loss_clf2: 0.0184, Loss_discrepancy: 0.0048
Train - Iteration 350: 	Loss_clf1: 0.0182, Loss_clf2: 0.0180, Loss_discrepancy: 0.0045
Train - Iteration 400: 	Loss_clf1: 0.0163, Loss_clf2: 0.0162, Loss_discrepancy: 0.0046
Train - Iteration 450: 	Loss_clf1: 0.0196, Loss_clf2: 0.0199, Loss_discrepancy: 0.0046
Train - Epoch [15]: 		Loss_clf1: 0.0177, Loss_clf2: 0.0177, Loss_discrepancy: 0.0047
Test - Epoch [15]: Accuracy_clf1: 11.19%, Accuracy_clf2: 11.16%, Accuracy_ensemble: 11.17%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0002, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0180, Loss_clf2: 0.0191, Loss_discrepancy: 0.0048
Train - Iteration 100: 	Loss_clf1: 0.0149, Loss_clf2: 0.0145, Loss_discrepancy: 0.0043
Train - Iteration 150: 	Loss_clf1: 0.0139, Loss_clf2: 0.0144, Loss_discrepancy: 0.0045
Train - Iteration 200: 	Loss_clf1: 0.0116, Loss_clf2: 0.0122, Loss_discrepancy: 0.0049
Train - Iteration 250: 	Loss_clf1: 0.0178, Loss_clf2: 0.0176, Loss_discrepancy: 0.0047
Train - Iteration 300: 	Loss_clf1: 0.0162, Loss_clf2: 0.0156, Loss_discrepancy: 0.0048
Train - Iteration 350: 	Loss_clf1: 0.0157, Loss_clf2: 0.0149, Loss_discrepancy: 0.0050
Train - Iteration 400: 	Loss_clf1: 0.0200, Loss_clf2: 0.0202, Loss_discrepancy: 0.0048
Train - Iteration 450: 	Loss_clf1: 0.0197, Loss_clf2: 0.0201, Loss_discrepancy: 0.0044
Train - Epoch [16]: 		Loss_clf1: 0.0166, Loss_clf2: 0.0168, Loss_discrepancy: 0.0047
Test - Epoch [16]: Accuracy_clf1: 11.33%, Accuracy_clf2: 11.35%, Accuracy_ensemble: 11.34%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0002, Loss_clf2: 0.0002, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0132, Loss_clf2: 0.0135, Loss_discrepancy: 0.0043
Train - Iteration 100: 	Loss_clf1: 0.0167, Loss_clf2: 0.0158, Loss_discrepancy: 0.0041
Train - Iteration 150: 	Loss_clf1: 0.0208, Loss_clf2: 0.0211, Loss_discrepancy: 0.0047
Train - Iteration 200: 	Loss_clf1: 0.0149, Loss_clf2: 0.0159, Loss_discrepancy: 0.0043
Train - Iteration 250: 	Loss_clf1: 0.0197, Loss_clf2: 0.0201, Loss_discrepancy: 0.0043
Train - Iteration 300: 	Loss_clf1: 0.0172, Loss_clf2: 0.0174, Loss_discrepancy: 0.0043
Train - Iteration 350: 	Loss_clf1: 0.0165, Loss_clf2: 0.0174, Loss_discrepancy: 0.0042
Train - Iteration 400: 	Loss_clf1: 0.0206, Loss_clf2: 0.0203, Loss_discrepancy: 0.0044
Train - Iteration 450: 	Loss_clf1: 0.0206, Loss_clf2: 0.0186, Loss_discrepancy: 0.0040
Train - Epoch [17]: 		Loss_clf1: 0.0180, Loss_clf2: 0.0180, Loss_discrepancy: 0.0043
Test - Epoch [17]: Accuracy_clf1: 10.54%, Accuracy_clf2: 10.49%, Accuracy_ensemble: 10.54%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0002, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0128, Loss_clf2: 0.0129, Loss_discrepancy: 0.0041
Train - Iteration 100: 	Loss_clf1: 0.0110, Loss_clf2: 0.0117, Loss_discrepancy: 0.0043
Train - Iteration 150: 	Loss_clf1: 0.0166, Loss_clf2: 0.0159, Loss_discrepancy: 0.0041
Train - Iteration 200: 	Loss_clf1: 0.0163, Loss_clf2: 0.0164, Loss_discrepancy: 0.0043
Train - Iteration 250: 	Loss_clf1: 0.0131, Loss_clf2: 0.0132, Loss_discrepancy: 0.0042
Train - Iteration 300: 	Loss_clf1: 0.0187, Loss_clf2: 0.0185, Loss_discrepancy: 0.0043
Train - Iteration 350: 	Loss_clf1: 0.0246, Loss_clf2: 0.0239, Loss_discrepancy: 0.0043
Train - Iteration 400: 	Loss_clf1: 0.0125, Loss_clf2: 0.0125, Loss_discrepancy: 0.0042
Train - Iteration 450: 	Loss_clf1: 0.0142, Loss_clf2: 0.0153, Loss_discrepancy: 0.0041
Train - Epoch [18]: 		Loss_clf1: 0.0154, Loss_clf2: 0.0154, Loss_discrepancy: 0.0042
Test - Epoch [18]: Accuracy_clf1: 11.00%, Accuracy_clf2: 10.90%, Accuracy_ensemble: 10.91%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0002, Loss_clf2: 0.0002, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0120, Loss_clf2: 0.0109, Loss_discrepancy: 0.0044
Train - Iteration 100: 	Loss_clf1: 0.0171, Loss_clf2: 0.0165, Loss_discrepancy: 0.0041
Train - Iteration 150: 	Loss_clf1: 0.0115, Loss_clf2: 0.0115, Loss_discrepancy: 0.0038
Train - Iteration 200: 	Loss_clf1: 0.0207, Loss_clf2: 0.0198, Loss_discrepancy: 0.0040
Train - Iteration 250: 	Loss_clf1: 0.0130, Loss_clf2: 0.0126, Loss_discrepancy: 0.0041
Train - Iteration 300: 	Loss_clf1: 0.0143, Loss_clf2: 0.0141, Loss_discrepancy: 0.0040
Train - Iteration 350: 	Loss_clf1: 0.0166, Loss_clf2: 0.0161, Loss_discrepancy: 0.0039
Train - Iteration 400: 	Loss_clf1: 0.0166, Loss_clf2: 0.0173, Loss_discrepancy: 0.0041
Train - Iteration 450: 	Loss_clf1: 0.0158, Loss_clf2: 0.0159, Loss_discrepancy: 0.0038
Train - Epoch [19]: 		Loss_clf1: 0.0153, Loss_clf2: 0.0151, Loss_discrepancy: 0.0040
Test - Epoch [19]: Accuracy_clf1: 10.44%, Accuracy_clf2: 10.44%, Accuracy_ensemble: 10.46%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0096, Loss_clf2: 0.0090, Loss_discrepancy: 0.0036
Train - Iteration 100: 	Loss_clf1: 0.0130, Loss_clf2: 0.0126, Loss_discrepancy: 0.0037
Train - Iteration 150: 	Loss_clf1: 0.0098, Loss_clf2: 0.0098, Loss_discrepancy: 0.0040
Train - Iteration 200: 	Loss_clf1: 0.0150, Loss_clf2: 0.0151, Loss_discrepancy: 0.0039
Train - Iteration 250: 	Loss_clf1: 0.0197, Loss_clf2: 0.0190, Loss_discrepancy: 0.0043
Train - Iteration 300: 	Loss_clf1: 0.0194, Loss_clf2: 0.0199, Loss_discrepancy: 0.0040
Train - Iteration 350: 	Loss_clf1: 0.0167, Loss_clf2: 0.0165, Loss_discrepancy: 0.0040
Train - Iteration 400: 	Loss_clf1: 0.0176, Loss_clf2: 0.0166, Loss_discrepancy: 0.0040
Train - Iteration 450: 	Loss_clf1: 0.0162, Loss_clf2: 0.0148, Loss_discrepancy: 0.0038
Train - Epoch [20]: 		Loss_clf1: 0.0153, Loss_clf2: 0.0149, Loss_discrepancy: 0.0039
Test - Epoch [20]: Accuracy_clf1: 10.59%, Accuracy_clf2: 10.53%, Accuracy_ensemble: 10.56%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0010, Loss_clf2: 0.0006, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0144, Loss_clf2: 0.0151, Loss_discrepancy: 0.0038
Train - Iteration 100: 	Loss_clf1: 0.0092, Loss_clf2: 0.0087, Loss_discrepancy: 0.0037
Train - Iteration 150: 	Loss_clf1: 0.0144, Loss_clf2: 0.0143, Loss_discrepancy: 0.0039
Train - Iteration 200: 	Loss_clf1: 0.0134, Loss_clf2: 0.0136, Loss_discrepancy: 0.0038
Train - Iteration 250: 	Loss_clf1: 0.0127, Loss_clf2: 0.0128, Loss_discrepancy: 0.0040
Train - Iteration 300: 	Loss_clf1: 0.0157, Loss_clf2: 0.0157, Loss_discrepancy: 0.0040
Train - Iteration 350: 	Loss_clf1: 0.0188, Loss_clf2: 0.0181, Loss_discrepancy: 0.0040
Train - Iteration 400: 	Loss_clf1: 0.0154, Loss_clf2: 0.0154, Loss_discrepancy: 0.0041
Train - Iteration 450: 	Loss_clf1: 0.0147, Loss_clf2: 0.0138, Loss_discrepancy: 0.0036
Train - Epoch [21]: 		Loss_clf1: 0.0147, Loss_clf2: 0.0145, Loss_discrepancy: 0.0039
Test - Epoch [21]: Accuracy_clf1: 10.22%, Accuracy_clf2: 10.22%, Accuracy_ensemble: 10.25%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0014, Loss_clf2: 0.0013, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0170, Loss_clf2: 0.0166, Loss_discrepancy: 0.0038
Train - Iteration 100: 	Loss_clf1: 0.0110, Loss_clf2: 0.0118, Loss_discrepancy: 0.0036
Train - Iteration 150: 	Loss_clf1: 0.0168, Loss_clf2: 0.0164, Loss_discrepancy: 0.0038
Train - Iteration 200: 	Loss_clf1: 0.0109, Loss_clf2: 0.0105, Loss_discrepancy: 0.0037
Train - Iteration 250: 	Loss_clf1: 0.0158, Loss_clf2: 0.0159, Loss_discrepancy: 0.0038
Train - Iteration 300: 	Loss_clf1: 0.0119, Loss_clf2: 0.0111, Loss_discrepancy: 0.0039
Train - Iteration 350: 	Loss_clf1: 0.0144, Loss_clf2: 0.0142, Loss_discrepancy: 0.0040
Train - Iteration 400: 	Loss_clf1: 0.0226, Loss_clf2: 0.0226, Loss_discrepancy: 0.0041
Train - Iteration 450: 	Loss_clf1: 0.0184, Loss_clf2: 0.0178, Loss_discrepancy: 0.0038
Train - Epoch [22]: 		Loss_clf1: 0.0156, Loss_clf2: 0.0153, Loss_discrepancy: 0.0038
Test - Epoch [22]: Accuracy_clf1: 10.57%, Accuracy_clf2: 10.55%, Accuracy_ensemble: 10.56%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0113, Loss_clf2: 0.0119, Loss_discrepancy: 0.0037
Train - Iteration 100: 	Loss_clf1: 0.0110, Loss_clf2: 0.0104, Loss_discrepancy: 0.0037
Train - Iteration 150: 	Loss_clf1: 0.0141, Loss_clf2: 0.0140, Loss_discrepancy: 0.0037
Train - Iteration 200: 	Loss_clf1: 0.0188, Loss_clf2: 0.0180, Loss_discrepancy: 0.0043
Train - Iteration 250: 	Loss_clf1: 0.0131, Loss_clf2: 0.0137, Loss_discrepancy: 0.0041
Train - Iteration 300: 	Loss_clf1: 0.0127, Loss_clf2: 0.0126, Loss_discrepancy: 0.0037
Train - Iteration 350: 	Loss_clf1: 0.0114, Loss_clf2: 0.0105, Loss_discrepancy: 0.0040
Train - Iteration 400: 	Loss_clf1: 0.0142, Loss_clf2: 0.0137, Loss_discrepancy: 0.0037
Train - Iteration 450: 	Loss_clf1: 0.0152, Loss_clf2: 0.0146, Loss_discrepancy: 0.0039
Train - Epoch [23]: 		Loss_clf1: 0.0139, Loss_clf2: 0.0137, Loss_discrepancy: 0.0039
Test - Epoch [23]: Accuracy_clf1: 10.41%, Accuracy_clf2: 10.41%, Accuracy_ensemble: 10.43%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0002, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0137, Loss_clf2: 0.0140, Loss_discrepancy: 0.0039
Train - Iteration 100: 	Loss_clf1: 0.0127, Loss_clf2: 0.0116, Loss_discrepancy: 0.0039
Train - Iteration 150: 	Loss_clf1: 0.0195, Loss_clf2: 0.0188, Loss_discrepancy: 0.0046
Train - Iteration 200: 	Loss_clf1: 0.0145, Loss_clf2: 0.0131, Loss_discrepancy: 0.0042
Train - Iteration 250: 	Loss_clf1: 0.0129, Loss_clf2: 0.0136, Loss_discrepancy: 0.0041
Train - Iteration 300: 	Loss_clf1: 0.0160, Loss_clf2: 0.0152, Loss_discrepancy: 0.0044
Train - Iteration 350: 	Loss_clf1: 0.0182, Loss_clf2: 0.0182, Loss_discrepancy: 0.0044
Train - Iteration 400: 	Loss_clf1: 0.0125, Loss_clf2: 0.0116, Loss_discrepancy: 0.0043
Train - Iteration 450: 	Loss_clf1: 0.0128, Loss_clf2: 0.0120, Loss_discrepancy: 0.0040
Train - Epoch [24]: 		Loss_clf1: 0.0148, Loss_clf2: 0.0143, Loss_discrepancy: 0.0042
Test - Epoch [24]: Accuracy_clf1: 10.26%, Accuracy_clf2: 10.34%, Accuracy_ensemble: 10.32%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0002, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0117, Loss_clf2: 0.0108, Loss_discrepancy: 0.0040
Train - Iteration 100: 	Loss_clf1: 0.0135, Loss_clf2: 0.0128, Loss_discrepancy: 0.0037
Train - Iteration 150: 	Loss_clf1: 0.0137, Loss_clf2: 0.0141, Loss_discrepancy: 0.0039
Train - Iteration 200: 	Loss_clf1: 0.0143, Loss_clf2: 0.0141, Loss_discrepancy: 0.0036
Train - Iteration 250: 	Loss_clf1: 0.0153, Loss_clf2: 0.0156, Loss_discrepancy: 0.0042
Train - Iteration 300: 	Loss_clf1: 0.0163, Loss_clf2: 0.0162, Loss_discrepancy: 0.0040
Train - Iteration 350: 	Loss_clf1: 0.0129, Loss_clf2: 0.0108, Loss_discrepancy: 0.0037
Train - Iteration 400: 	Loss_clf1: 0.0099, Loss_clf2: 0.0105, Loss_discrepancy: 0.0036
Train - Iteration 450: 	Loss_clf1: 0.0163, Loss_clf2: 0.0153, Loss_discrepancy: 0.0040
Train - Epoch [25]: 		Loss_clf1: 0.0140, Loss_clf2: 0.0135, Loss_discrepancy: 0.0039
Test - Epoch [25]: Accuracy_clf1: 9.95%, Accuracy_clf2: 9.96%, Accuracy_ensemble: 9.95%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0002, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0100, Loss_clf2: 0.0099, Loss_discrepancy: 0.0037
Train - Iteration 100: 	Loss_clf1: 0.0121, Loss_clf2: 0.0114, Loss_discrepancy: 0.0036
Train - Iteration 150: 	Loss_clf1: 0.0079, Loss_clf2: 0.0078, Loss_discrepancy: 0.0037
Train - Iteration 200: 	Loss_clf1: 0.0112, Loss_clf2: 0.0106, Loss_discrepancy: 0.0037
Train - Iteration 250: 	Loss_clf1: 0.0165, Loss_clf2: 0.0159, Loss_discrepancy: 0.0038
Train - Iteration 300: 	Loss_clf1: 0.0139, Loss_clf2: 0.0137, Loss_discrepancy: 0.0039
Train - Iteration 350: 	Loss_clf1: 0.0152, Loss_clf2: 0.0140, Loss_discrepancy: 0.0035
Train - Iteration 400: 	Loss_clf1: 0.0119, Loss_clf2: 0.0113, Loss_discrepancy: 0.0035
Train - Iteration 450: 	Loss_clf1: 0.0141, Loss_clf2: 0.0140, Loss_discrepancy: 0.0036
Train - Epoch [26]: 		Loss_clf1: 0.0125, Loss_clf2: 0.0121, Loss_discrepancy: 0.0037
Test - Epoch [26]: Accuracy_clf1: 10.72%, Accuracy_clf2: 10.68%, Accuracy_ensemble: 10.74%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0118, Loss_clf2: 0.0122, Loss_discrepancy: 0.0038
Train - Iteration 100: 	Loss_clf1: 0.0116, Loss_clf2: 0.0121, Loss_discrepancy: 0.0038
Train - Iteration 150: 	Loss_clf1: 0.0121, Loss_clf2: 0.0119, Loss_discrepancy: 0.0042
Train - Iteration 200: 	Loss_clf1: 0.0146, Loss_clf2: 0.0140, Loss_discrepancy: 0.0039
Train - Iteration 250: 	Loss_clf1: 0.0102, Loss_clf2: 0.0098, Loss_discrepancy: 0.0035
Train - Iteration 300: 	Loss_clf1: 0.0108, Loss_clf2: 0.0107, Loss_discrepancy: 0.0038
Train - Iteration 350: 	Loss_clf1: 0.0173, Loss_clf2: 0.0167, Loss_discrepancy: 0.0040
Train - Iteration 400: 	Loss_clf1: 0.0158, Loss_clf2: 0.0149, Loss_discrepancy: 0.0038
Train - Iteration 450: 	Loss_clf1: 0.0131, Loss_clf2: 0.0130, Loss_discrepancy: 0.0037
Train - Epoch [27]: 		Loss_clf1: 0.0131, Loss_clf2: 0.0129, Loss_discrepancy: 0.0038
Test - Epoch [27]: Accuracy_clf1: 10.20%, Accuracy_clf2: 10.24%, Accuracy_ensemble: 10.23%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0002, Loss_clf2: 0.0002, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0118, Loss_clf2: 0.0122, Loss_discrepancy: 0.0035
Train - Iteration 100: 	Loss_clf1: 0.0108, Loss_clf2: 0.0105, Loss_discrepancy: 0.0041
Train - Iteration 150: 	Loss_clf1: 0.0158, Loss_clf2: 0.0149, Loss_discrepancy: 0.0037
Train - Iteration 200: 	Loss_clf1: 0.0124, Loss_clf2: 0.0124, Loss_discrepancy: 0.0042
Train - Iteration 250: 	Loss_clf1: 0.0118, Loss_clf2: 0.0111, Loss_discrepancy: 0.0039
Train - Iteration 300: 	Loss_clf1: 0.0080, Loss_clf2: 0.0081, Loss_discrepancy: 0.0040
Train - Iteration 350: 	Loss_clf1: 0.0146, Loss_clf2: 0.0143, Loss_discrepancy: 0.0039
Train - Iteration 400: 	Loss_clf1: 0.0156, Loss_clf2: 0.0156, Loss_discrepancy: 0.0039
Train - Iteration 450: 	Loss_clf1: 0.0169, Loss_clf2: 0.0170, Loss_discrepancy: 0.0038
Train - Epoch [28]: 		Loss_clf1: 0.0132, Loss_clf2: 0.0130, Loss_discrepancy: 0.0039
Test - Epoch [28]: Accuracy_clf1: 10.11%, Accuracy_clf2: 10.09%, Accuracy_ensemble: 10.08%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0002, Loss_clf2: 0.0002, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0120, Loss_clf2: 0.0112, Loss_discrepancy: 0.0035
Train - Iteration 100: 	Loss_clf1: 0.0110, Loss_clf2: 0.0101, Loss_discrepancy: 0.0040
Train - Iteration 150: 	Loss_clf1: 0.0112, Loss_clf2: 0.0106, Loss_discrepancy: 0.0037
Train - Iteration 200: 	Loss_clf1: 0.0114, Loss_clf2: 0.0113, Loss_discrepancy: 0.0040
Train - Iteration 250: 	Loss_clf1: 0.0181, Loss_clf2: 0.0178, Loss_discrepancy: 0.0040
Train - Iteration 300: 	Loss_clf1: 0.0177, Loss_clf2: 0.0183, Loss_discrepancy: 0.0044
Train - Iteration 350: 	Loss_clf1: 0.0161, Loss_clf2: 0.0156, Loss_discrepancy: 0.0039
Train - Iteration 400: 	Loss_clf1: 0.0131, Loss_clf2: 0.0128, Loss_discrepancy: 0.0039
Train - Iteration 450: 	Loss_clf1: 0.0115, Loss_clf2: 0.0109, Loss_discrepancy: 0.0039
Train - Epoch [29]: 		Loss_clf1: 0.0135, Loss_clf2: 0.0132, Loss_discrepancy: 0.0039
Test - Epoch [29]: Accuracy_clf1: 10.15%, Accuracy_clf2: 10.16%, Accuracy_ensemble: 10.20%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0002, Loss_clf2: 0.0003, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0135, Loss_clf2: 0.0123, Loss_discrepancy: 0.0038
Train - Iteration 100: 	Loss_clf1: 0.0138, Loss_clf2: 0.0145, Loss_discrepancy: 0.0037
Train - Iteration 150: 	Loss_clf1: 0.0123, Loss_clf2: 0.0117, Loss_discrepancy: 0.0037
Train - Iteration 200: 	Loss_clf1: 0.0153, Loss_clf2: 0.0148, Loss_discrepancy: 0.0032
Train - Iteration 250: 	Loss_clf1: 0.0105, Loss_clf2: 0.0108, Loss_discrepancy: 0.0035
Train - Iteration 300: 	Loss_clf1: 0.0141, Loss_clf2: 0.0132, Loss_discrepancy: 0.0034
Train - Iteration 350: 	Loss_clf1: 0.0138, Loss_clf2: 0.0142, Loss_discrepancy: 0.0034
Train - Iteration 400: 	Loss_clf1: 0.0126, Loss_clf2: 0.0120, Loss_discrepancy: 0.0032
Train - Iteration 450: 	Loss_clf1: 0.0125, Loss_clf2: 0.0123, Loss_discrepancy: 0.0036
Train - Epoch [30]: 		Loss_clf1: 0.0130, Loss_clf2: 0.0127, Loss_discrepancy: 0.0035
Test - Epoch [30]: Accuracy_clf1: 10.18%, Accuracy_clf2: 10.20%, Accuracy_ensemble: 10.22%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0000, Loss_clf2: 0.0000, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0139, Loss_clf2: 0.0135, Loss_discrepancy: 0.0033
Train - Iteration 100: 	Loss_clf1: 0.0089, Loss_clf2: 0.0084, Loss_discrepancy: 0.0034
Train - Iteration 150: 	Loss_clf1: 0.0121, Loss_clf2: 0.0121, Loss_discrepancy: 0.0038
Train - Iteration 200: 	Loss_clf1: 0.0080, Loss_clf2: 0.0079, Loss_discrepancy: 0.0040
Train - Iteration 250: 	Loss_clf1: 0.0145, Loss_clf2: 0.0139, Loss_discrepancy: 0.0039
Train - Iteration 300: 	Loss_clf1: 0.0145, Loss_clf2: 0.0141, Loss_discrepancy: 0.0037
Train - Iteration 350: 	Loss_clf1: 0.0119, Loss_clf2: 0.0126, Loss_discrepancy: 0.0037
Train - Iteration 400: 	Loss_clf1: 0.0129, Loss_clf2: 0.0129, Loss_discrepancy: 0.0037
Train - Iteration 450: 	Loss_clf1: 0.0110, Loss_clf2: 0.0114, Loss_discrepancy: 0.0034
Train - Epoch [31]: 		Loss_clf1: 0.0121, Loss_clf2: 0.0120, Loss_discrepancy: 0.0037
Test - Epoch [31]: Accuracy_clf1: 10.10%, Accuracy_clf2: 10.12%, Accuracy_ensemble: 10.08%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0107, Loss_clf2: 0.0106, Loss_discrepancy: 0.0033
Train - Iteration 100: 	Loss_clf1: 0.0108, Loss_clf2: 0.0116, Loss_discrepancy: 0.0034
Train - Iteration 150: 	Loss_clf1: 0.0105, Loss_clf2: 0.0104, Loss_discrepancy: 0.0035
Train - Iteration 200: 	Loss_clf1: 0.0119, Loss_clf2: 0.0124, Loss_discrepancy: 0.0038
Train - Iteration 250: 	Loss_clf1: 0.0184, Loss_clf2: 0.0179, Loss_discrepancy: 0.0038
Train - Iteration 300: 	Loss_clf1: 0.0130, Loss_clf2: 0.0124, Loss_discrepancy: 0.0036
Train - Iteration 350: 	Loss_clf1: 0.0098, Loss_clf2: 0.0106, Loss_discrepancy: 0.0038
Train - Iteration 400: 	Loss_clf1: 0.0156, Loss_clf2: 0.0161, Loss_discrepancy: 0.0038
Train - Iteration 450: 	Loss_clf1: 0.0100, Loss_clf2: 0.0091, Loss_discrepancy: 0.0036
Train - Epoch [32]: 		Loss_clf1: 0.0125, Loss_clf2: 0.0125, Loss_discrepancy: 0.0036
Test - Epoch [32]: Accuracy_clf1: 10.96%, Accuracy_clf2: 10.95%, Accuracy_ensemble: 10.96%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0129, Loss_clf2: 0.0132, Loss_discrepancy: 0.0035
Train - Iteration 100: 	Loss_clf1: 0.0119, Loss_clf2: 0.0113, Loss_discrepancy: 0.0036
Train - Iteration 150: 	Loss_clf1: 0.0087, Loss_clf2: 0.0085, Loss_discrepancy: 0.0038
Train - Iteration 200: 	Loss_clf1: 0.0094, Loss_clf2: 0.0092, Loss_discrepancy: 0.0038
Train - Iteration 250: 	Loss_clf1: 0.0123, Loss_clf2: 0.0125, Loss_discrepancy: 0.0040
Train - Iteration 300: 	Loss_clf1: 0.0109, Loss_clf2: 0.0113, Loss_discrepancy: 0.0038
Train - Iteration 350: 	Loss_clf1: 0.0138, Loss_clf2: 0.0147, Loss_discrepancy: 0.0038
Train - Iteration 400: 	Loss_clf1: 0.0122, Loss_clf2: 0.0122, Loss_discrepancy: 0.0035
Train - Iteration 450: 	Loss_clf1: 0.0156, Loss_clf2: 0.0145, Loss_discrepancy: 0.0034
Train - Epoch [33]: 		Loss_clf1: 0.0117, Loss_clf2: 0.0117, Loss_discrepancy: 0.0037
Test - Epoch [33]: Accuracy_clf1: 9.89%, Accuracy_clf2: 9.92%, Accuracy_ensemble: 9.86%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0105, Loss_clf2: 0.0103, Loss_discrepancy: 0.0037
Train - Iteration 100: 	Loss_clf1: 0.0109, Loss_clf2: 0.0107, Loss_discrepancy: 0.0039
Train - Iteration 150: 	Loss_clf1: 0.0129, Loss_clf2: 0.0133, Loss_discrepancy: 0.0037
Train - Iteration 200: 	Loss_clf1: 0.0115, Loss_clf2: 0.0117, Loss_discrepancy: 0.0033
Train - Iteration 250: 	Loss_clf1: 0.0058, Loss_clf2: 0.0059, Loss_discrepancy: 0.0034
Train - Iteration 300: 	Loss_clf1: 0.0129, Loss_clf2: 0.0127, Loss_discrepancy: 0.0035
Train - Iteration 350: 	Loss_clf1: 0.0134, Loss_clf2: 0.0134, Loss_discrepancy: 0.0036
Train - Iteration 400: 	Loss_clf1: 0.0136, Loss_clf2: 0.0130, Loss_discrepancy: 0.0036
Train - Iteration 450: 	Loss_clf1: 0.0143, Loss_clf2: 0.0144, Loss_discrepancy: 0.0036
Train - Epoch [34]: 		Loss_clf1: 0.0118, Loss_clf2: 0.0118, Loss_discrepancy: 0.0036
Test - Epoch [34]: Accuracy_clf1: 10.60%, Accuracy_clf2: 10.64%, Accuracy_ensemble: 10.62%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0092, Loss_clf2: 0.0090, Loss_discrepancy: 0.0036
Train - Iteration 100: 	Loss_clf1: 0.0120, Loss_clf2: 0.0117, Loss_discrepancy: 0.0037
Train - Iteration 150: 	Loss_clf1: 0.0129, Loss_clf2: 0.0115, Loss_discrepancy: 0.0040
Train - Iteration 200: 	Loss_clf1: 0.0120, Loss_clf2: 0.0126, Loss_discrepancy: 0.0040
Train - Iteration 250: 	Loss_clf1: 0.0154, Loss_clf2: 0.0152, Loss_discrepancy: 0.0037
Train - Iteration 300: 	Loss_clf1: 0.0144, Loss_clf2: 0.0145, Loss_discrepancy: 0.0034
Train - Iteration 350: 	Loss_clf1: 0.0137, Loss_clf2: 0.0137, Loss_discrepancy: 0.0036
Train - Iteration 400: 	Loss_clf1: 0.0109, Loss_clf2: 0.0109, Loss_discrepancy: 0.0036
Train - Iteration 450: 	Loss_clf1: 0.0110, Loss_clf2: 0.0102, Loss_discrepancy: 0.0037
Train - Epoch [35]: 		Loss_clf1: 0.0126, Loss_clf2: 0.0123, Loss_discrepancy: 0.0037
Test - Epoch [35]: Accuracy_clf1: 10.46%, Accuracy_clf2: 10.50%, Accuracy_ensemble: 10.52%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0003, Loss_clf2: 0.0003, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0101, Loss_clf2: 0.0109, Loss_discrepancy: 0.0041
Train - Iteration 100: 	Loss_clf1: 0.0107, Loss_clf2: 0.0103, Loss_discrepancy: 0.0035
Train - Iteration 150: 	Loss_clf1: 0.0073, Loss_clf2: 0.0070, Loss_discrepancy: 0.0036
Train - Iteration 200: 	Loss_clf1: 0.0112, Loss_clf2: 0.0118, Loss_discrepancy: 0.0039
Train - Iteration 250: 	Loss_clf1: 0.0126, Loss_clf2: 0.0130, Loss_discrepancy: 0.0038
Train - Iteration 300: 	Loss_clf1: 0.0078, Loss_clf2: 0.0073, Loss_discrepancy: 0.0038
Train - Iteration 350: 	Loss_clf1: 0.0101, Loss_clf2: 0.0104, Loss_discrepancy: 0.0036
Train - Iteration 400: 	Loss_clf1: 0.0104, Loss_clf2: 0.0107, Loss_discrepancy: 0.0038
Train - Iteration 450: 	Loss_clf1: 0.0156, Loss_clf2: 0.0161, Loss_discrepancy: 0.0036
Train - Epoch [36]: 		Loss_clf1: 0.0106, Loss_clf2: 0.0108, Loss_discrepancy: 0.0038
Test - Epoch [36]: Accuracy_clf1: 9.83%, Accuracy_clf2: 9.83%, Accuracy_ensemble: 9.82%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0000, Loss_clf2: 0.0000, Loss_discrepancy: 0.0000
Train - Iteration 50: 	Loss_clf1: 0.0078, Loss_clf2: 0.0074, Loss_discrepancy: 0.0037
Train - Iteration 100: 	Loss_clf1: 0.0096, Loss_clf2: 0.0097, Loss_discrepancy: 0.0036
Train - Iteration 150: 	Loss_clf1: 0.0103, Loss_clf2: 0.0096, Loss_discrepancy: 0.0037
Train - Iteration 200: 	Loss_clf1: 0.0115, Loss_clf2: 0.0112, Loss_discrepancy: 0.0034
Train - Iteration 250: 	Loss_clf1: 0.0138, Loss_clf2: 0.0134, Loss_discrepancy: 0.0034
Train - Iteration 300: 	Loss_clf1: 0.0109, Loss_clf2: 0.0101, Loss_discrepancy: 0.0033
Train - Iteration 350: 	Loss_clf1: 0.0167, Loss_clf2: 0.0160, Loss_discrepancy: 0.0036
Train - Iteration 400: 	Loss_clf1: 0.0105, Loss_clf2: 0.0101, Loss_discrepancy: 0.0035
Train - Iteration 450: 	Loss_clf1: 0.0115, Loss_clf2: 0.0117, Loss_discrepancy: 0.0035
Train - Epoch [37]: 		Loss_clf1: 0.0116, Loss_clf2: 0.0113, Loss_discrepancy: 0.0035
Test - Epoch [37]: Accuracy_clf1: 9.78%, Accuracy_clf2: 9.80%, Accuracy_ensemble: 9.77%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0081, Loss_clf2: 0.0082, Loss_discrepancy: 0.0035
Train - Iteration 100: 	Loss_clf1: 0.0064, Loss_clf2: 0.0060, Loss_discrepancy: 0.0035
Train - Iteration 150: 	Loss_clf1: 0.0096, Loss_clf2: 0.0097, Loss_discrepancy: 0.0034
Train - Iteration 200: 	Loss_clf1: 0.0110, Loss_clf2: 0.0108, Loss_discrepancy: 0.0039
Train - Iteration 250: 	Loss_clf1: 0.0166, Loss_clf2: 0.0164, Loss_discrepancy: 0.0040
Train - Iteration 300: 	Loss_clf1: 0.0098, Loss_clf2: 0.0106, Loss_discrepancy: 0.0034
Train - Iteration 350: 	Loss_clf1: 0.0114, Loss_clf2: 0.0112, Loss_discrepancy: 0.0035
Train - Iteration 400: 	Loss_clf1: 0.0090, Loss_clf2: 0.0085, Loss_discrepancy: 0.0033
Train - Iteration 450: 	Loss_clf1: 0.0159, Loss_clf2: 0.0161, Loss_discrepancy: 0.0037
Train - Epoch [38]: 		Loss_clf1: 0.0110, Loss_clf2: 0.0110, Loss_discrepancy: 0.0036
Test - Epoch [38]: Accuracy_clf1: 10.31%, Accuracy_clf2: 10.32%, Accuracy_ensemble: 10.27%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0085, Loss_clf2: 0.0087, Loss_discrepancy: 0.0039
Train - Iteration 100: 	Loss_clf1: 0.0103, Loss_clf2: 0.0104, Loss_discrepancy: 0.0039
Train - Iteration 150: 	Loss_clf1: 0.0151, Loss_clf2: 0.0159, Loss_discrepancy: 0.0040
Train - Iteration 200: 	Loss_clf1: 0.0113, Loss_clf2: 0.0100, Loss_discrepancy: 0.0036
Train - Iteration 250: 	Loss_clf1: 0.0097, Loss_clf2: 0.0097, Loss_discrepancy: 0.0039
Train - Iteration 300: 	Loss_clf1: 0.0157, Loss_clf2: 0.0155, Loss_discrepancy: 0.0041
Train - Iteration 350: 	Loss_clf1: 0.0147, Loss_clf2: 0.0147, Loss_discrepancy: 0.0038
Train - Iteration 400: 	Loss_clf1: 0.0160, Loss_clf2: 0.0150, Loss_discrepancy: 0.0040
Train - Iteration 450: 	Loss_clf1: 0.0120, Loss_clf2: 0.0121, Loss_discrepancy: 0.0036
Train - Epoch [39]: 		Loss_clf1: 0.0124, Loss_clf2: 0.0123, Loss_discrepancy: 0.0039
Test - Epoch [39]: Accuracy_clf1: 10.23%, Accuracy_clf2: 10.32%, Accuracy_ensemble: 10.29%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0000, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0086, Loss_clf2: 0.0080, Loss_discrepancy: 0.0038
Train - Iteration 100: 	Loss_clf1: 0.0079, Loss_clf2: 0.0072, Loss_discrepancy: 0.0036
Train - Iteration 150: 	Loss_clf1: 0.0131, Loss_clf2: 0.0131, Loss_discrepancy: 0.0036
Train - Iteration 200: 	Loss_clf1: 0.0099, Loss_clf2: 0.0105, Loss_discrepancy: 0.0037
Train - Iteration 250: 	Loss_clf1: 0.0132, Loss_clf2: 0.0125, Loss_discrepancy: 0.0036
Train - Iteration 300: 	Loss_clf1: 0.0074, Loss_clf2: 0.0068, Loss_discrepancy: 0.0040
Train - Iteration 350: 	Loss_clf1: 0.0113, Loss_clf2: 0.0116, Loss_discrepancy: 0.0037
Train - Iteration 400: 	Loss_clf1: 0.0116, Loss_clf2: 0.0118, Loss_discrepancy: 0.0037
Train - Iteration 450: 	Loss_clf1: 0.0125, Loss_clf2: 0.0125, Loss_discrepancy: 0.0035
Train - Epoch [40]: 		Loss_clf1: 0.0107, Loss_clf2: 0.0105, Loss_discrepancy: 0.0037
Test - Epoch [40]: Accuracy_clf1: 10.48%, Accuracy_clf2: 10.45%, Accuracy_ensemble: 10.43%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0006, Loss_clf2: 0.0005, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0113, Loss_clf2: 0.0108, Loss_discrepancy: 0.0035
Train - Iteration 100: 	Loss_clf1: 0.0104, Loss_clf2: 0.0103, Loss_discrepancy: 0.0033
Train - Iteration 150: 	Loss_clf1: 0.0112, Loss_clf2: 0.0107, Loss_discrepancy: 0.0033
Train - Iteration 200: 	Loss_clf1: 0.0134, Loss_clf2: 0.0135, Loss_discrepancy: 0.0034
Train - Iteration 250: 	Loss_clf1: 0.0098, Loss_clf2: 0.0108, Loss_discrepancy: 0.0033
Train - Iteration 300: 	Loss_clf1: 0.0095, Loss_clf2: 0.0094, Loss_discrepancy: 0.0033
Train - Iteration 350: 	Loss_clf1: 0.0080, Loss_clf2: 0.0079, Loss_discrepancy: 0.0035
Train - Iteration 400: 	Loss_clf1: 0.0101, Loss_clf2: 0.0092, Loss_discrepancy: 0.0036
Train - Iteration 450: 	Loss_clf1: 0.0096, Loss_clf2: 0.0102, Loss_discrepancy: 0.0034
Train - Epoch [41]: 		Loss_clf1: 0.0105, Loss_clf2: 0.0104, Loss_discrepancy: 0.0034
Test - Epoch [41]: Accuracy_clf1: 10.30%, Accuracy_clf2: 10.31%, Accuracy_ensemble: 10.27%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0121, Loss_clf2: 0.0113, Loss_discrepancy: 0.0033
Train - Iteration 100: 	Loss_clf1: 0.0113, Loss_clf2: 0.0112, Loss_discrepancy: 0.0035
Train - Iteration 150: 	Loss_clf1: 0.0059, Loss_clf2: 0.0060, Loss_discrepancy: 0.0030
Train - Iteration 200: 	Loss_clf1: 0.0109, Loss_clf2: 0.0106, Loss_discrepancy: 0.0036
Train - Iteration 250: 	Loss_clf1: 0.0121, Loss_clf2: 0.0109, Loss_discrepancy: 0.0034
Train - Iteration 300: 	Loss_clf1: 0.0096, Loss_clf2: 0.0088, Loss_discrepancy: 0.0034
Train - Iteration 350: 	Loss_clf1: 0.0114, Loss_clf2: 0.0126, Loss_discrepancy: 0.0038
Train - Iteration 400: 	Loss_clf1: 0.0074, Loss_clf2: 0.0071, Loss_discrepancy: 0.0030
Train - Iteration 450: 	Loss_clf1: 0.0118, Loss_clf2: 0.0117, Loss_discrepancy: 0.0034
Train - Epoch [42]: 		Loss_clf1: 0.0102, Loss_clf2: 0.0099, Loss_discrepancy: 0.0034
Test - Epoch [42]: Accuracy_clf1: 10.34%, Accuracy_clf2: 10.32%, Accuracy_ensemble: 10.32%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0002, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0075, Loss_clf2: 0.0074, Loss_discrepancy: 0.0036
Train - Iteration 100: 	Loss_clf1: 0.0120, Loss_clf2: 0.0121, Loss_discrepancy: 0.0039
Train - Iteration 150: 	Loss_clf1: 0.0112, Loss_clf2: 0.0120, Loss_discrepancy: 0.0037
Train - Iteration 200: 	Loss_clf1: 0.0097, Loss_clf2: 0.0097, Loss_discrepancy: 0.0032
Train - Iteration 250: 	Loss_clf1: 0.0114, Loss_clf2: 0.0108, Loss_discrepancy: 0.0034
Train - Iteration 300: 	Loss_clf1: 0.0109, Loss_clf2: 0.0109, Loss_discrepancy: 0.0032
Train - Iteration 350: 	Loss_clf1: 0.0121, Loss_clf2: 0.0113, Loss_discrepancy: 0.0032
Train - Iteration 400: 	Loss_clf1: 0.0117, Loss_clf2: 0.0122, Loss_discrepancy: 0.0037
Train - Iteration 450: 	Loss_clf1: 0.0095, Loss_clf2: 0.0098, Loss_discrepancy: 0.0036
Train - Epoch [43]: 		Loss_clf1: 0.0107, Loss_clf2: 0.0107, Loss_discrepancy: 0.0035
Test - Epoch [43]: Accuracy_clf1: 10.81%, Accuracy_clf2: 10.77%, Accuracy_ensemble: 10.79%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0000
Train - Iteration 50: 	Loss_clf1: 0.0104, Loss_clf2: 0.0107, Loss_discrepancy: 0.0034
Train - Iteration 100: 	Loss_clf1: 0.0078, Loss_clf2: 0.0072, Loss_discrepancy: 0.0039
Train - Iteration 150: 	Loss_clf1: 0.0123, Loss_clf2: 0.0123, Loss_discrepancy: 0.0036
Train - Iteration 200: 	Loss_clf1: 0.0127, Loss_clf2: 0.0123, Loss_discrepancy: 0.0039
Train - Iteration 250: 	Loss_clf1: 0.0129, Loss_clf2: 0.0128, Loss_discrepancy: 0.0034
Train - Iteration 300: 	Loss_clf1: 0.0106, Loss_clf2: 0.0108, Loss_discrepancy: 0.0035
Train - Iteration 350: 	Loss_clf1: 0.0094, Loss_clf2: 0.0098, Loss_discrepancy: 0.0040
Train - Iteration 400: 	Loss_clf1: 0.0118, Loss_clf2: 0.0117, Loss_discrepancy: 0.0035
Train - Iteration 450: 	Loss_clf1: 0.0107, Loss_clf2: 0.0110, Loss_discrepancy: 0.0037
Train - Epoch [44]: 		Loss_clf1: 0.0111, Loss_clf2: 0.0111, Loss_discrepancy: 0.0036
Test - Epoch [44]: Accuracy_clf1: 10.73%, Accuracy_clf2: 10.75%, Accuracy_ensemble: 10.76%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0000, Loss_clf2: 0.0000, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0081, Loss_clf2: 0.0086, Loss_discrepancy: 0.0038
Train - Iteration 100: 	Loss_clf1: 0.0072, Loss_clf2: 0.0077, Loss_discrepancy: 0.0042
Train - Iteration 150: 	Loss_clf1: 0.0092, Loss_clf2: 0.0084, Loss_discrepancy: 0.0039
Train - Iteration 200: 	Loss_clf1: 0.0078, Loss_clf2: 0.0077, Loss_discrepancy: 0.0044
Train - Iteration 250: 	Loss_clf1: 0.0083, Loss_clf2: 0.0090, Loss_discrepancy: 0.0045
Train - Iteration 300: 	Loss_clf1: 0.0099, Loss_clf2: 0.0105, Loss_discrepancy: 0.0045
Train - Iteration 350: 	Loss_clf1: 0.0125, Loss_clf2: 0.0134, Loss_discrepancy: 0.0044
Train - Iteration 400: 	Loss_clf1: 0.0100, Loss_clf2: 0.0112, Loss_discrepancy: 0.0039
Train - Iteration 450: 	Loss_clf1: 0.0105, Loss_clf2: 0.0106, Loss_discrepancy: 0.0038
Train - Epoch [45]: 		Loss_clf1: 0.0094, Loss_clf2: 0.0098, Loss_discrepancy: 0.0042
Test - Epoch [45]: Accuracy_clf1: 10.88%, Accuracy_clf2: 10.79%, Accuracy_ensemble: 10.85%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0002, Loss_clf2: 0.0002, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0063, Loss_clf2: 0.0062, Loss_discrepancy: 0.0042
Train - Iteration 100: 	Loss_clf1: 0.0113, Loss_clf2: 0.0118, Loss_discrepancy: 0.0042
Train - Iteration 150: 	Loss_clf1: 0.0110, Loss_clf2: 0.0120, Loss_discrepancy: 0.0052
Train - Iteration 200: 	Loss_clf1: 0.0127, Loss_clf2: 0.0129, Loss_discrepancy: 0.0058
Train - Iteration 250: 	Loss_clf1: 0.0146, Loss_clf2: 0.0151, Loss_discrepancy: 0.0059
Train - Iteration 300: 	Loss_clf1: 0.0107, Loss_clf2: 0.0117, Loss_discrepancy: 0.0052
Train - Iteration 350: 	Loss_clf1: 0.0089, Loss_clf2: 0.0088, Loss_discrepancy: 0.0051
Train - Iteration 400: 	Loss_clf1: 0.0110, Loss_clf2: 0.0113, Loss_discrepancy: 0.0050
Train - Iteration 450: 	Loss_clf1: 0.0124, Loss_clf2: 0.0134, Loss_discrepancy: 0.0058
Train - Epoch [46]: 		Loss_clf1: 0.0109, Loss_clf2: 0.0114, Loss_discrepancy: 0.0052
Test - Epoch [46]: Accuracy_clf1: 9.89%, Accuracy_clf2: 9.85%, Accuracy_ensemble: 9.86%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0000, Loss_clf2: 0.0000, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0086, Loss_clf2: 0.0093, Loss_discrepancy: 0.0072
Train - Iteration 100: 	Loss_clf1: 0.0105, Loss_clf2: 0.0122, Loss_discrepancy: 0.0092
Train - Iteration 150: 	Loss_clf1: 0.0094, Loss_clf2: 0.0094, Loss_discrepancy: 0.0059
Train - Iteration 200: 	Loss_clf1: 0.0114, Loss_clf2: 0.0116, Loss_discrepancy: 0.0054
Train - Iteration 250: 	Loss_clf1: 0.0078, Loss_clf2: 0.0078, Loss_discrepancy: 0.0055
Train - Iteration 300: 	Loss_clf1: 0.0133, Loss_clf2: 0.0136, Loss_discrepancy: 0.0053
Train - Iteration 350: 	Loss_clf1: 0.0120, Loss_clf2: 0.0118, Loss_discrepancy: 0.0046
Train - Iteration 400: 	Loss_clf1: 0.0073, Loss_clf2: 0.0078, Loss_discrepancy: 0.0045
Train - Iteration 450: 	Loss_clf1: 0.0104, Loss_clf2: 0.0108, Loss_discrepancy: 0.0045
Train - Epoch [47]: 		Loss_clf1: 0.0100, Loss_clf2: 0.0105, Loss_discrepancy: 0.0057
Test - Epoch [47]: Accuracy_clf1: 10.31%, Accuracy_clf2: 10.40%, Accuracy_ensemble: 10.40%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0118, Loss_clf2: 0.0132, Loss_discrepancy: 0.0049
Train - Iteration 100: 	Loss_clf1: 0.0076, Loss_clf2: 0.0079, Loss_discrepancy: 0.0040
Train - Iteration 150: 	Loss_clf1: 0.0106, Loss_clf2: 0.0098, Loss_discrepancy: 0.0040
Train - Iteration 200: 	Loss_clf1: 0.0097, Loss_clf2: 0.0098, Loss_discrepancy: 0.0046
Train - Iteration 250: 	Loss_clf1: 0.0081, Loss_clf2: 0.0089, Loss_discrepancy: 0.0040
Train - Iteration 300: 	Loss_clf1: 0.0100, Loss_clf2: 0.0115, Loss_discrepancy: 0.0039
Train - Iteration 350: 	Loss_clf1: 0.0117, Loss_clf2: 0.0108, Loss_discrepancy: 0.0038
Train - Iteration 400: 	Loss_clf1: 0.0128, Loss_clf2: 0.0141, Loss_discrepancy: 0.0036
Train - Iteration 450: 	Loss_clf1: 0.0117, Loss_clf2: 0.0130, Loss_discrepancy: 0.0041
Train - Epoch [48]: 		Loss_clf1: 0.0108, Loss_clf2: 0.0113, Loss_discrepancy: 0.0041
Test - Epoch [48]: Accuracy_clf1: 10.40%, Accuracy_clf2: 10.46%, Accuracy_ensemble: 10.39%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0001, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0091, Loss_clf2: 0.0098, Loss_discrepancy: 0.0039
Train - Iteration 100: 	Loss_clf1: 0.0070, Loss_clf2: 0.0075, Loss_discrepancy: 0.0038
Train - Iteration 150: 	Loss_clf1: 0.0121, Loss_clf2: 0.0119, Loss_discrepancy: 0.0036
Train - Iteration 200: 	Loss_clf1: 0.0097, Loss_clf2: 0.0093, Loss_discrepancy: 0.0037
Train - Iteration 250: 	Loss_clf1: 0.0079, Loss_clf2: 0.0083, Loss_discrepancy: 0.0036
Train - Iteration 300: 	Loss_clf1: 0.0066, Loss_clf2: 0.0069, Loss_discrepancy: 0.0039
Train - Iteration 350: 	Loss_clf1: 0.0091, Loss_clf2: 0.0094, Loss_discrepancy: 0.0040
Train - Iteration 400: 	Loss_clf1: 0.0160, Loss_clf2: 0.0162, Loss_discrepancy: 0.0041
Train - Iteration 450: 	Loss_clf1: 0.0115, Loss_clf2: 0.0109, Loss_discrepancy: 0.0039
Train - Epoch [49]: 		Loss_clf1: 0.0098, Loss_clf2: 0.0100, Loss_discrepancy: 0.0039
Test - Epoch [49]: Accuracy_clf1: 9.74%, Accuracy_clf2: 9.83%, Accuracy_ensemble: 9.76%
-----------------------------------------------------------------------------------------------

Train - Iteration 0: 	Loss_clf1: 0.0001, Loss_clf2: 0.0002, Loss_discrepancy: 0.0001
Train - Iteration 50: 	Loss_clf1: 0.0097, Loss_clf2: 0.0107, Loss_discrepancy: 0.0042
Train - Iteration 100: 	Loss_clf1: 0.0107, Loss_clf2: 0.0110, Loss_discrepancy: 0.0046
Train - Iteration 150: 	Loss_clf1: 0.0106, Loss_clf2: 0.0097, Loss_discrepancy: 0.0041
Train - Iteration 200: 	Loss_clf1: 0.0085, Loss_clf2: 0.0087, Loss_discrepancy: 0.0045
Train - Iteration 250: 	Loss_clf1: 0.0091, Loss_clf2: 0.0099, Loss_discrepancy: 0.0045
Train - Iteration 300: 	Loss_clf1: 0.0130, Loss_clf2: 0.0128, Loss_discrepancy: 0.0038
Train - Iteration 350: 	Loss_clf1: 0.0097, Loss_clf2: 0.0098, Loss_discrepancy: 0.0039
Train - Iteration 400: 	Loss_clf1: 0.0103, Loss_clf2: 0.0104, Loss_discrepancy: 0.0040
Train - Iteration 450: 	Loss_clf1: 0.0091, Loss_clf2: 0.0094, Loss_discrepancy: 0.0038
Train - Epoch [50]: 		Loss_clf1: 0.0100, Loss_clf2: 0.0102, Loss_discrepancy: 0.0042
Test - Epoch [50]: Accuracy_clf1: 10.13%, Accuracy_clf2: 10.25%, Accuracy_ensemble: 10.19%
-----------------------------------------------------------------------------------------------

The process took 46.48925097783407 minutes to complete.
