nohup: ignoring input
Using downloaded and verified file: ./data/SVHN/raw/train_32x32.mat
Using downloaded and verified file: ./data/SVHN/raw/test_32x32.mat
Using device cuda
Training starts
Epoch [1/10] starts
Iteration 50: Loss_G: 2.3279, Loss_D: 0.1225
Iteration 100: Loss_G: 1.7183, Loss_D: 0.2006
Iteration 150: Loss_G: 1.2968, Loss_D: 0.2070
Iteration 200: Loss_G: 1.3843, Loss_D: 0.1983
Iteration 250: Loss_G: 1.3453, Loss_D: 0.2041
Iteration 300: Loss_G: 1.3546, Loss_D: 0.2036
Iteration 350: Loss_G: 1.3574, Loss_D: 0.2032
Iteration 400: Loss_G: 1.2917, Loss_D: 0.2096
Iteration 450: Loss_G: 1.2771, Loss_D: 0.2106
Iteration 500: Loss_G: 1.2997, Loss_D: 0.2084
Iteration 550: Loss_G: 1.2551, Loss_D: 0.2084
Iteration 600: Loss_G: 1.2710, Loss_D: 0.2085
Iteration 650: Loss_G: 1.2724, Loss_D: 0.2024
Iteration 700: Loss_G: 1.2709, Loss_D: 0.2080
Iteration 750: Loss_G: 1.2448, Loss_D: 0.2102
Iteration 800: Loss_G: 1.2583, Loss_D: 0.2104
Iteration 850: Loss_G: 1.2415, Loss_D: 0.2124
Iteration 900: Loss_G: 1.2316, Loss_D: 0.2135
Iteration 950: Loss_G: 1.2332, Loss_D: 0.2170
Iteration 1000: Loss_G: 1.2318, Loss_D: 0.2110
Iteration 1050: Loss_G: 1.2714, Loss_D: 0.2125
Iteration 1100: Loss_G: 1.2314, Loss_D: 0.2136
Iteration 1150: Loss_G: 1.2286, Loss_D: 0.2162
Iteration 1200: Loss_G: 1.2374, Loss_D: 0.2131
Epoch [1/10], Loss_G: 1.0959, Loss_D: 0.2052
Epoch [2/10] starts
Iteration 50: Loss_G: 1.2802, Loss_D: 0.2126
Iteration 100: Loss_G: 1.2463, Loss_D: 0.2172
Iteration 150: Loss_G: 1.2308, Loss_D: 0.2165
Iteration 200: Loss_G: 1.1978, Loss_D: 0.2143
Iteration 250: Loss_G: 1.2327, Loss_D: 0.2166
Iteration 300: Loss_G: 1.2216, Loss_D: 0.2129
Iteration 350: Loss_G: 1.2279, Loss_D: 0.2162
Iteration 400: Loss_G: 1.2407, Loss_D: 0.2140
Iteration 450: Loss_G: 1.2581, Loss_D: 0.2103
Iteration 500: Loss_G: 1.1814, Loss_D: 0.2151
Iteration 550: Loss_G: 1.2431, Loss_D: 0.2133
Iteration 600: Loss_G: 1.2179, Loss_D: 0.2150
Iteration 650: Loss_G: 1.2431, Loss_D: 0.2127
Iteration 700: Loss_G: 1.1929, Loss_D: 0.2152
Iteration 750: Loss_G: 1.2221, Loss_D: 0.2096
Iteration 800: Loss_G: 1.2324, Loss_D: 0.2110
Iteration 850: Loss_G: 1.2253, Loss_D: 0.2126
Iteration 900: Loss_G: 1.2321, Loss_D: 0.2140
Iteration 950: Loss_G: 1.2124, Loss_D: 0.2121
Iteration 1000: Loss_G: 1.2526, Loss_D: 0.2071
Iteration 1050: Loss_G: 1.2468, Loss_D: 0.2125
Iteration 1100: Loss_G: 1.2399, Loss_D: 0.2181
Iteration 1150: Loss_G: 1.2103, Loss_D: 0.2146
Iteration 1200: Loss_G: 1.1578, Loss_D: 0.2194
Epoch [2/10], Loss_G: 1.0043, Loss_D: 0.2139
Epoch [3/10] starts
Iteration 50: Loss_G: 1.2584, Loss_D: 0.2102
Iteration 100: Loss_G: 1.2430, Loss_D: 0.2090
Iteration 150: Loss_G: 1.2852, Loss_D: 0.2098
Iteration 200: Loss_G: 1.2390, Loss_D: 0.2072
Iteration 250: Loss_G: 1.2418, Loss_D: 0.2102
Iteration 300: Loss_G: 1.2439, Loss_D: 0.2104
Iteration 350: Loss_G: 1.2327, Loss_D: 0.2098
Iteration 400: Loss_G: 1.2556, Loss_D: 0.2094
Iteration 450: Loss_G: 1.2925, Loss_D: 0.2061
Iteration 500: Loss_G: 1.2454, Loss_D: 0.2070
Iteration 550: Loss_G: 1.2808, Loss_D: 0.2080
Iteration 600: Loss_G: 1.2278, Loss_D: 0.2108
Iteration 650: Loss_G: 1.2612, Loss_D: 0.2084
Iteration 700: Loss_G: 1.2393, Loss_D: 0.2120
Iteration 750: Loss_G: 1.2157, Loss_D: 0.2127
Iteration 800: Loss_G: 1.2349, Loss_D: 0.2099
Iteration 850: Loss_G: 1.2238, Loss_D: 0.2077
Iteration 900: Loss_G: 1.2434, Loss_D: 0.2122
Iteration 950: Loss_G: 1.2358, Loss_D: 0.2086
Iteration 1000: Loss_G: 1.2479, Loss_D: 0.2120
Iteration 1050: Loss_G: 1.2091, Loss_D: 0.2109
Iteration 1100: Loss_G: 1.2836, Loss_D: 0.2034
Iteration 1150: Loss_G: 1.2670, Loss_D: 0.2079
Iteration 1200: Loss_G: 1.2262, Loss_D: 0.2144
Epoch [3/10], Loss_G: 1.0209, Loss_D: 0.2095
Epoch [4/10] starts
Iteration 50: Loss_G: 1.2530, Loss_D: 0.2091
Iteration 100: Loss_G: 1.2343, Loss_D: 0.2088
Iteration 150: Loss_G: 1.2233, Loss_D: 0.2103
Iteration 200: Loss_G: 1.2632, Loss_D: 0.2088
Iteration 250: Loss_G: 1.2218, Loss_D: 0.2124
Iteration 300: Loss_G: 1.2052, Loss_D: 0.2086
Iteration 350: Loss_G: 1.2734, Loss_D: 0.2065
Iteration 400: Loss_G: 1.2531, Loss_D: 0.2094
Iteration 450: Loss_G: 1.2220, Loss_D: 0.2143
Iteration 500: Loss_G: 1.2245, Loss_D: 0.2087
Iteration 550: Loss_G: 1.2390, Loss_D: 0.2116
Iteration 600: Loss_G: 1.2433, Loss_D: 0.2122
Iteration 650: Loss_G: 1.2502, Loss_D: 0.2080
Iteration 700: Loss_G: 1.2207, Loss_D: 0.2108
Iteration 750: Loss_G: 1.2225, Loss_D: 0.2123
Iteration 800: Loss_G: 1.2306, Loss_D: 0.2166
Iteration 850: Loss_G: 1.2264, Loss_D: 0.2108
Iteration 900: Loss_G: 1.2425, Loss_D: 0.2110
Iteration 950: Loss_G: 1.2462, Loss_D: 0.2065
Iteration 1000: Loss_G: 1.2789, Loss_D: 0.2038
Iteration 1050: Loss_G: 1.3304, Loss_D: 0.1947
Iteration 1100: Loss_G: 1.3073, Loss_D: 0.2022
Iteration 1150: Loss_G: 1.2720, Loss_D: 0.2085
Iteration 1200: Loss_G: 1.3230, Loss_D: 0.2026
Epoch [4/10], Loss_G: 1.0234, Loss_D: 0.2087
Epoch [5/10] starts
Iteration 50: Loss_G: 1.2725, Loss_D: 0.2065
Iteration 100: Loss_G: 1.3272, Loss_D: 0.1969
Iteration 150: Loss_G: 1.2772, Loss_D: 0.2075
Iteration 200: Loss_G: 1.3230, Loss_D: 0.2022
Iteration 250: Loss_G: 1.3717, Loss_D: 0.1951
Iteration 300: Loss_G: 1.2005, Loss_D: 0.2137
Iteration 350: Loss_G: 1.3623, Loss_D: 0.2026
Iteration 400: Loss_G: 1.2587, Loss_D: 0.2076
Iteration 450: Loss_G: 1.2753, Loss_D: 0.2032
Iteration 500: Loss_G: 1.2748, Loss_D: 0.2088
Iteration 550: Loss_G: 1.2722, Loss_D: 0.2023
Iteration 600: Loss_G: 1.2740, Loss_D: 0.2068
Iteration 650: Loss_G: 1.2812, Loss_D: 0.2052
Iteration 700: Loss_G: 1.2700, Loss_D: 0.2021
Iteration 750: Loss_G: 1.2697, Loss_D: 0.2055
Iteration 800: Loss_G: 1.2882, Loss_D: 0.2014
Iteration 850: Loss_G: 1.2932, Loss_D: 0.2007
Iteration 900: Loss_G: 1.2684, Loss_D: 0.2083
Iteration 950: Loss_G: 1.3007, Loss_D: 0.2052
Iteration 1000: Loss_G: 1.2370, Loss_D: 0.2104
Iteration 1050: Loss_G: 1.2635, Loss_D: 0.2047
Iteration 1100: Loss_G: 1.2609, Loss_D: 0.2039
Iteration 1150: Loss_G: 1.2529, Loss_D: 0.2049
Iteration 1200: Loss_G: 1.2658, Loss_D: 0.2053
Epoch [5/10], Loss_G: 1.0485, Loss_D: 0.2046
Epoch [6/10] starts
Iteration 50: Loss_G: 1.2497, Loss_D: 0.2023
Iteration 100: Loss_G: 1.2582, Loss_D: 0.2061
Iteration 150: Loss_G: 1.2670, Loss_D: 0.2052
Iteration 200: Loss_G: 1.2604, Loss_D: 0.2020
Iteration 250: Loss_G: 1.2885, Loss_D: 0.2057
Iteration 300: Loss_G: 1.2403, Loss_D: 0.2091
Iteration 350: Loss_G: 1.2824, Loss_D: 0.1998
Iteration 400: Loss_G: 1.2620, Loss_D: 0.2079
Iteration 450: Loss_G: 1.2731, Loss_D: 0.2072
Iteration 500: Loss_G: 1.2491, Loss_D: 0.2094
Iteration 550: Loss_G: 1.2589, Loss_D: 0.2020
Iteration 600: Loss_G: 1.2604, Loss_D: 0.2050
Iteration 650: Loss_G: 1.2471, Loss_D: 0.2088
Iteration 700: Loss_G: 1.2773, Loss_D: 0.2081
Iteration 750: Loss_G: 1.2396, Loss_D: 0.2093
Iteration 800: Loss_G: 1.2396, Loss_D: 0.2073
Iteration 850: Loss_G: 1.2427, Loss_D: 0.2059
Iteration 900: Loss_G: 1.2432, Loss_D: 0.2090
Iteration 950: Loss_G: 1.2140, Loss_D: 0.2080
Iteration 1000: Loss_G: 1.2658, Loss_D: 0.2014
Iteration 1050: Loss_G: 1.2377, Loss_D: 0.2104
Iteration 1100: Loss_G: 1.2293, Loss_D: 0.2090
Iteration 1150: Loss_G: 1.2117, Loss_D: 0.2099
Iteration 1200: Loss_G: 1.2257, Loss_D: 0.2074
Epoch [6/10], Loss_G: 1.0240, Loss_D: 0.2065
Epoch [7/10] starts
Iteration 50: Loss_G: 1.2395, Loss_D: 0.2049
Iteration 100: Loss_G: 1.2531, Loss_D: 0.2043
Iteration 150: Loss_G: 1.2709, Loss_D: 0.2029
Iteration 200: Loss_G: 1.2913, Loss_D: 0.2015
Iteration 250: Loss_G: 1.2621, Loss_D: 0.2072
Iteration 300: Loss_G: 1.2426, Loss_D: 0.2094
Iteration 350: Loss_G: 1.2213, Loss_D: 0.2058
Iteration 400: Loss_G: 1.2685, Loss_D: 0.2034
Iteration 450: Loss_G: 1.2549, Loss_D: 0.2057
Iteration 500: Loss_G: 1.2725, Loss_D: 0.2019
Iteration 550: Loss_G: 1.2307, Loss_D: 0.2063
Iteration 600: Loss_G: 1.2620, Loss_D: 0.2092
Iteration 650: Loss_G: 1.2547, Loss_D: 0.2034
Iteration 700: Loss_G: 1.2387, Loss_D: 0.2095
Iteration 750: Loss_G: 1.2477, Loss_D: 0.2044
Iteration 800: Loss_G: 1.2312, Loss_D: 0.2079
Iteration 850: Loss_G: 1.2347, Loss_D: 0.2073
Iteration 900: Loss_G: 1.2451, Loss_D: 0.2046
Iteration 950: Loss_G: 1.2673, Loss_D: 0.2051
Iteration 1000: Loss_G: 1.2571, Loss_D: 0.2029
Iteration 1050: Loss_G: 1.2298, Loss_D: 0.2098
Iteration 1100: Loss_G: 1.2337, Loss_D: 0.2085
Iteration 1150: Loss_G: 1.2794, Loss_D: 0.1999
Iteration 1200: Loss_G: 1.2547, Loss_D: 0.2056
Epoch [7/10], Loss_G: 1.0247, Loss_D: 0.2055
Epoch [8/10] starts
Iteration 50: Loss_G: 1.2458, Loss_D: 0.2050
Iteration 100: Loss_G: 1.2181, Loss_D: 0.2091
Iteration 150: Loss_G: 1.2539, Loss_D: 0.2036
Iteration 200: Loss_G: 1.2624, Loss_D: 0.2060
Iteration 250: Loss_G: 1.2206, Loss_D: 0.2083
Iteration 300: Loss_G: 1.2346, Loss_D: 0.2073
Iteration 350: Loss_G: 1.2026, Loss_D: 0.2118
Iteration 400: Loss_G: 1.2569, Loss_D: 0.1994
Iteration 450: Loss_G: 1.2674, Loss_D: 0.2033
Iteration 500: Loss_G: 1.2688, Loss_D: 0.2040
Iteration 550: Loss_G: 1.2613, Loss_D: 0.2057
Iteration 600: Loss_G: 1.2914, Loss_D: 0.2035
Iteration 650: Loss_G: 1.2328, Loss_D: 0.2124
Iteration 700: Loss_G: 1.2376, Loss_D: 0.2025
Iteration 750: Loss_G: 1.2706, Loss_D: 0.2034
Iteration 800: Loss_G: 1.2093, Loss_D: 0.2100
Iteration 850: Loss_G: 1.2534, Loss_D: 0.2070
Iteration 900: Loss_G: 1.2321, Loss_D: 0.2048
Iteration 950: Loss_G: 1.2441, Loss_D: 0.2065
Iteration 1000: Loss_G: 1.2215, Loss_D: 0.2064
Iteration 1050: Loss_G: 1.2674, Loss_D: 0.2012
Iteration 1100: Loss_G: 1.2325, Loss_D: 0.2090
Iteration 1150: Loss_G: 1.2404, Loss_D: 0.2056
Iteration 1200: Loss_G: 1.2118, Loss_D: 0.2103
Epoch [8/10], Loss_G: 1.0176, Loss_D: 0.2061
Epoch [9/10] starts
Iteration 50: Loss_G: 1.2560, Loss_D: 0.2039
Iteration 100: Loss_G: 1.2452, Loss_D: 0.2028
Iteration 150: Loss_G: 1.2343, Loss_D: 0.2064
Iteration 200: Loss_G: 1.2248, Loss_D: 0.2042
Iteration 250: Loss_G: 1.2553, Loss_D: 0.2011
Iteration 300: Loss_G: 1.2621, Loss_D: 0.2021
Iteration 350: Loss_G: 1.2081, Loss_D: 0.2076
Iteration 400: Loss_G: 1.2438, Loss_D: 0.2042
Iteration 450: Loss_G: 1.2414, Loss_D: 0.2036
Iteration 500: Loss_G: 1.2666, Loss_D: 0.2041
Iteration 550: Loss_G: 1.2234, Loss_D: 0.2082
Iteration 600: Loss_G: 1.2528, Loss_D: 0.2014
Iteration 650: Loss_G: 1.2710, Loss_D: 0.2027
Iteration 700: Loss_G: 1.2109, Loss_D: 0.2050
Iteration 750: Loss_G: 1.2669, Loss_D: 0.2023
Iteration 800: Loss_G: 1.2463, Loss_D: 0.2053
Iteration 850: Loss_G: 1.2189, Loss_D: 0.2042
Iteration 900: Loss_G: 1.2528, Loss_D: 0.2062
Iteration 950: Loss_G: 1.2485, Loss_D: 0.2040
Iteration 1000: Loss_G: 1.2270, Loss_D: 0.2071
Iteration 1050: Loss_G: 1.2405, Loss_D: 0.2064
Iteration 1100: Loss_G: 1.2229, Loss_D: 0.2080
Iteration 1150: Loss_G: 1.2155, Loss_D: 0.2067
Iteration 1200: Loss_G: 1.2344, Loss_D: 0.2073
Epoch [9/10], Loss_G: 1.0153, Loss_D: 0.2048
Epoch [10/10] starts
Iteration 50: Loss_G: 1.2426, Loss_D: 0.2024
Iteration 100: Loss_G: 1.2423, Loss_D: 0.2052
Iteration 150: Loss_G: 1.2347, Loss_D: 0.2051
Iteration 200: Loss_G: 1.2517, Loss_D: 0.2017
Iteration 250: Loss_G: 1.2867, Loss_D: 0.1992
Iteration 300: Loss_G: 1.2897, Loss_D: 0.1993
Iteration 350: Loss_G: 1.2730, Loss_D: 0.2031
Iteration 400: Loss_G: 1.2398, Loss_D: 0.2018
Iteration 450: Loss_G: 1.2674, Loss_D: 0.2025
Iteration 500: Loss_G: 1.2522, Loss_D: 0.2038
Iteration 550: Loss_G: 1.2849, Loss_D: 0.1976
Iteration 600: Loss_G: 1.2753, Loss_D: 0.2015
Iteration 650: Loss_G: 1.2382, Loss_D: 0.2040
Iteration 700: Loss_G: 1.2459, Loss_D: 0.2062
Iteration 750: Loss_G: 1.2542, Loss_D: 0.1988
Iteration 800: Loss_G: 1.2461, Loss_D: 0.2007
Iteration 850: Loss_G: 1.2642, Loss_D: 0.2041
Iteration 900: Loss_G: 1.2539, Loss_D: 0.2011
Iteration 950: Loss_G: 1.2901, Loss_D: 0.2001
Iteration 1000: Loss_G: 1.2762, Loss_D: 0.2027
Iteration 1050: Loss_G: 1.2730, Loss_D: 0.2046
Iteration 1100: Loss_G: 1.2389, Loss_D: 0.2029
Iteration 1150: Loss_G: 1.2382, Loss_D: 0.2046
Iteration 1200: Loss_G: 1.2267, Loss_D: 0.2045
Epoch [10/10], Loss_G: 1.0295, Loss_D: 0.2024
The process took 31.41621834039688 minutes to complete.
